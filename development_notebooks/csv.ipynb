{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edcf335d",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90f33b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cohirf.experiment.hpo_csv_clustering_experiment import HPOCSVClusteringExperiment\n",
    "from pathlib import Path\n",
    "from cohirf.models.batch_cohirf import BatchCoHiRF\n",
    "from cohirf.models.cohirf import BaseCoHiRF\n",
    "from sklearn.cluster import DBSCAN\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee51295f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_dir = Path(\"/home/belucci/code/cohirf/results\") / \"csv\"\n",
    "results_dir = Path(\"/home/users/belucci/cohirf/results/csv\")\n",
    "mlflow_tracking_uri = f\"sqlite:///{results_dir}/mlflow.db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d2a62fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_params = dict(\n",
    "    mlflow_tracking_uri=mlflow_tracking_uri,\n",
    "    check_if_exists=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b98adc",
   "metadata": {},
   "source": [
    "# CIFAR-100 Coarse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a2b3ec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "653aa274b9db432e99a36de3115b996d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Combinations completed:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/27 16:59:35 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2025/08/27 16:59:35 INFO mlflow.store.db.utils: Updating database tables\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0b9becbb4d549f4beebafef3587d845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Trials:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjusted_rand: 0.2942, HPO time: 981.08s, Best time: 11.34s\n"
     ]
    }
   ],
   "source": [
    "hpo_metric = \"adjusted_rand\"\n",
    "experiment = HPOCSVClusteringExperiment(\n",
    "    # hpo\n",
    "    n_trials=20,\n",
    "    hpo_seed=0,\n",
    "    hpo_metric=hpo_metric,\n",
    "    direction=\"maximize\",\n",
    "    # model\n",
    "    experiment_name=\"test-cifar-coarse\",\n",
    "    model=\"KMeans\",\n",
    "    seed_model=0,\n",
    "    # dataset\n",
    "    dataset_name=\"cifar-100-coarse\",\n",
    "    raise_on_error=True,\n",
    "    standardize=True,\n",
    "    verbose=1,\n",
    "    **experiment_params,\n",
    ")\n",
    "result = experiment.run(return_results=True)[0]\n",
    "metric = result[\"evaluate_model_return\"][f\"best/{hpo_metric}\"]\n",
    "hpo_time = result[\"fit_model_return\"][\"elapsed_time\"]\n",
    "best_time = result[\"evaluate_model_return\"][\"best/elapsed_time\"]\n",
    "print(f\"{hpo_metric}: {metric:.4f}, HPO time: {hpo_time:.2f}s, Best time: {best_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdf50637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cc1cac6f0284e3cbb5be4f2ea6ec924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Combinations completed:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68e3301a3ea24eb9b09a30f6380dd9bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Trials:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjusted_rand: 0.2326, HPO time: 1004.19s, Best time: 8.10s\n"
     ]
    }
   ],
   "source": [
    "hpo_metric = \"adjusted_rand\"\n",
    "experiment = HPOCSVClusteringExperiment(\n",
    "    # hpo\n",
    "    n_trials=20,\n",
    "    hpo_seed=0,\n",
    "    hpo_metric=hpo_metric,\n",
    "    direction=\"maximize\",\n",
    "    # model\n",
    "    experiment_name=\"test-cifar-coarse\",\n",
    "    model=\"BatchCoHiRF-1iter\",\n",
    "\tn_jobs=10,\n",
    "    seed_model=0,\n",
    "    # dataset\n",
    "    dataset_name=\"cifar-100-coarse\",\n",
    "    raise_on_error=True,\n",
    "    standardize=True,\n",
    "    verbose=1,\n",
    "    **experiment_params,\n",
    ")\n",
    "result = experiment.run(return_results=True)[0]\n",
    "metric = result[\"evaluate_model_return\"][f\"best/{hpo_metric}\"]\n",
    "hpo_time = result[\"fit_model_return\"][\"elapsed_time\"]\n",
    "best_time = result[\"evaluate_model_return\"][\"best/elapsed_time\"]\n",
    "print(f\"{hpo_metric}: {metric:.4f}, HPO time: {hpo_time:.2f}s, Best time: {best_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1130726e",
   "metadata": {},
   "source": [
    "# Spotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a79ee379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e448385f66b24217a06855c49f41113d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Combinations completed:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84f038f83b8e4e74b32caa62c273e4d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Trials:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjusted_rand: 0.0630, HPO time: 27.19s, Best time: 0.17s\n"
     ]
    }
   ],
   "source": [
    "hpo_metric = \"adjusted_rand\"\n",
    "experiment = HPOCSVClusteringExperiment(\n",
    "    # hpo\n",
    "    n_trials=20,\n",
    "    hpo_seed=0,\n",
    "    hpo_metric=hpo_metric,\n",
    "    direction=\"maximize\",\n",
    "    # model\n",
    "    experiment_name=\"test-spotify\",\n",
    "    model=\"KMeans\",\n",
    "    seed_model=0,\n",
    "    # dataset\n",
    "    dataset_name=\"spotify\",\n",
    "    raise_on_error=True,\n",
    "    standardize=True,\n",
    "    verbose=1,\n",
    "    **experiment_params,\n",
    ")\n",
    "result = experiment.run(return_results=True)[0]\n",
    "metric = result[\"evaluate_model_return\"][f\"best/{hpo_metric}\"]\n",
    "hpo_time = result[\"fit_model_return\"][\"elapsed_time\"]\n",
    "best_time = result[\"evaluate_model_return\"][\"best/elapsed_time\"]\n",
    "print(f\"{hpo_metric}: {metric:.4f}, HPO time: {hpo_time:.2f}s, Best time: {best_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2a2a7f1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6734e1cf8f9d4c34be120533134ceb05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Combinations completed:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c976d24dcd454505beaa62d6f3a79920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Trials:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjusted_mutual_info: 0.1154, HPO time: 186.47s, Best time: 3.84s\n"
     ]
    }
   ],
   "source": [
    "hpo_metric = \"adjusted_mutual_info\"\n",
    "experiment = HPOCSVClusteringExperiment(\n",
    "    # hpo\n",
    "    n_trials=20,\n",
    "    hpo_seed=0,\n",
    "    hpo_metric=hpo_metric,\n",
    "    direction=\"maximize\",\n",
    "    # model\n",
    "    experiment_name=\"test-spotify\",\n",
    "    model=\"KernelRBFKMeans\",\n",
    "    seed_model=0,\n",
    "    # dataset\n",
    "    dataset_name=\"spotify\",\n",
    "    raise_on_error=True,\n",
    "    standardize=True,\n",
    "    verbose=1,\n",
    "    **experiment_params,\n",
    ")\n",
    "result = experiment.run(return_results=True)[0]\n",
    "metric = result[\"evaluate_model_return\"][f\"best/{hpo_metric}\"]\n",
    "hpo_time = result[\"fit_model_return\"][\"elapsed_time\"]\n",
    "best_time = result[\"evaluate_model_return\"][\"best/elapsed_time\"]\n",
    "print(f\"{hpo_metric}: {metric:.4f}, HPO time: {hpo_time:.2f}s, Best time: {best_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c79c47c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c5cbb35e0844fd583c46d5a9799ab96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Combinations completed:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7bda64d1e524ad8881ece3dc97fc4ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Trials:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "experiment = HPOCSVClusteringExperiment(\n",
    "    # hpo\n",
    "    n_trials=20,\n",
    "    hpo_seed=0,\n",
    "    hpo_metric=\"adjusted_rand\",\n",
    "    direction=\"maximize\",\n",
    "    # model\n",
    "    experiment_name=\"test-spotify\",\n",
    "    model=\"DBSCAN\",\n",
    "    seed_model=0,\n",
    "    # dataset\n",
    "    dataset_name=\"spotify\",\n",
    "    raise_on_error=True,\n",
    "    standardize=True,\n",
    "    verbose=1,\n",
    "    **experiment_params,\n",
    ")\n",
    "result = experiment.run(return_results=True)[0]\n",
    "ari = result[\"evaluate_model_return\"][\"best/adjusted_rand\"]\n",
    "hpo_time = result[\"fit_model_return\"][\"elapsed_time\"]\n",
    "best_time = result[\"evaluate_model_return\"][\"best/elapsed_time\"]\n",
    "print(f\"ARI: {ari:.4f}, HPO time: {hpo_time:.2f}s, Best time: {best_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fb9fa5bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c380871eef804b5aa77f568cdee7eb16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Combinations completed:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90d604d66d44476b87038e98325124f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Trials:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/belucci/code/cohirf/cohirf/experiment/clustering_experiment.py:270: UserWarning: Too many clusters (73091) for dataset with 74290 samples. Skipping metric calculation. If you want to calculate metrics anyway, set `calculate_metrics_even_if_too_many_clusters` to True.\n",
      "  warn(f\"Too many clusters ({n_clusters}) for dataset with {X.shape[0]} samples. Skipping metric calculation. If you want to calculate metrics anyway, set `calculate_metrics_even_if_too_many_clusters` to True.\")\n",
      "/home/belucci/code/ml_experiments/ml_experiments/tuners.py:233: UserWarning: metric adjusted_mutual_info not found in dict returned by training_fn, available metrics are dict_keys(['n_clusters_', 'elapsed_time'])\n",
      "  warn(f'metric {metric} not found in dict returned by training_fn, available metrics are '\n",
      "/home/belucci/code/cohirf/cohirf/experiment/clustering_experiment.py:270: UserWarning: Too many clusters (58475) for dataset with 74290 samples. Skipping metric calculation. If you want to calculate metrics anyway, set `calculate_metrics_even_if_too_many_clusters` to True.\n",
      "  warn(f\"Too many clusters ({n_clusters}) for dataset with {X.shape[0]} samples. Skipping metric calculation. If you want to calculate metrics anyway, set `calculate_metrics_even_if_too_many_clusters` to True.\")\n",
      "/home/belucci/code/ml_experiments/ml_experiments/tuners.py:233: UserWarning: metric adjusted_mutual_info not found in dict returned by training_fn, available metrics are dict_keys(['n_clusters_', 'elapsed_time'])\n",
      "  warn(f'metric {metric} not found in dict returned by training_fn, available metrics are '\n",
      "/home/belucci/code/cohirf/cohirf/experiment/clustering_experiment.py:270: UserWarning: Too many clusters (49648) for dataset with 74290 samples. Skipping metric calculation. If you want to calculate metrics anyway, set `calculate_metrics_even_if_too_many_clusters` to True.\n",
      "  warn(f\"Too many clusters ({n_clusters}) for dataset with {X.shape[0]} samples. Skipping metric calculation. If you want to calculate metrics anyway, set `calculate_metrics_even_if_too_many_clusters` to True.\")\n",
      "/home/belucci/code/ml_experiments/ml_experiments/tuners.py:233: UserWarning: metric adjusted_mutual_info not found in dict returned by training_fn, available metrics are dict_keys(['n_clusters_', 'elapsed_time'])\n",
      "  warn(f'metric {metric} not found in dict returned by training_fn, available metrics are '\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m      1\u001b[39m hpo_metric = \u001b[33m\"\u001b[39m\u001b[33madjusted_mutual_info\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      2\u001b[39m experiment = HPOCSVClusteringExperiment(\n\u001b[32m      3\u001b[39m     \u001b[38;5;66;03m# hpo\u001b[39;00m\n\u001b[32m      4\u001b[39m     n_trials=\u001b[32m20\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     19\u001b[39m     **experiment_params,\n\u001b[32m     20\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m result = \u001b[43mexperiment\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreturn_results\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m     22\u001b[39m metric = result[\u001b[33m\"\u001b[39m\u001b[33mevaluate_model_return\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbest/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhpo_metric\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m]\n\u001b[32m     23\u001b[39m hpo_time = result[\u001b[33m\"\u001b[39m\u001b[33mfit_model_return\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33melapsed_time\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/ml_experiments/ml_experiments/base_experiment.py:1571\u001b[39m, in \u001b[36mBaseExperiment.run\u001b[39m\u001b[34m(self, return_results)\u001b[39m\n\u001b[32m   1569\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1570\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1571\u001b[39m     results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1572\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/ml_experiments/ml_experiments/base_experiment.py:1503\u001b[39m, in \u001b[36mBaseExperiment._run_experiment\u001b[39m\u001b[34m(self, client, return_results)\u001b[39m\n\u001b[32m   1501\u001b[39m     combination = \u001b[38;5;28mlist\u001b[39m(combination) + [run_id]\n\u001b[32m   1502\u001b[39m     combination_names.append(\u001b[33m\"\u001b[39m\u001b[33mmlflow_run_id\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1503\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_combination\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1504\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43mcombination\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1505\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcombination_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcombination_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1506\u001b[39m \u001b[43m    \u001b[49m\u001b[43munique_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43munique_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1507\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextra_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1508\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1509\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_results:\n\u001b[32m   1511\u001b[39m     combination_success = result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/ml_experiments/ml_experiments/base_experiment.py:1300\u001b[39m, in \u001b[36mBaseExperiment._run_combination\u001b[39m\u001b[34m(self, combination_names, unique_params, extra_params, return_results, *combination, **kwargs)\u001b[39m\n\u001b[32m   1298\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1299\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1300\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs_fn\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/ml_experiments/ml_experiments/base_experiment.py:1247\u001b[39m, in \u001b[36mBaseExperiment._run_mlflow_and_train_model\u001b[39m\u001b[34m(self, combination, unique_params, extra_params, return_results, mlflow_run_id, **kwargs)\u001b[39m\n\u001b[32m   1237\u001b[39m mlflow_client.update_run(mlflow_run_id, status=\u001b[33m\"\u001b[39m\u001b[33mRUNNING\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1238\u001b[39m \u001b[38;5;28mself\u001b[39m._log_run_start_params(\n\u001b[32m   1239\u001b[39m     combination=combination,\n\u001b[32m   1240\u001b[39m     unique_params=unique_params,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1244\u001b[39m     **kwargs,\n\u001b[32m   1245\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1247\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1248\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcombination\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcombination\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1249\u001b[39m \u001b[43m    \u001b[49m\u001b[43munique_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43munique_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1250\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1251\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextra_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1252\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmlflow_run_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmlflow_run_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1253\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1254\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/ml_experiments/ml_experiments/base_experiment.py:1010\u001b[39m, in \u001b[36mBaseExperiment._train_model\u001b[39m\u001b[34m(self, combination, unique_params, extra_params, return_results, mlflow_run_id, **kwargs)\u001b[39m\n\u001b[32m   1008\u001b[39m         results[\u001b[33m\"\u001b[39m\u001b[33mfit_model_return\u001b[39m\u001b[33m\"\u001b[39m] = ret\n\u001b[32m   1009\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1010\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcombination\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcombination\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43munique_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43munique_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmlflow_run_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmlflow_run_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1018\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ret:\n\u001b[32m   1019\u001b[39m         results[\u001b[33m\"\u001b[39m\u001b[33mfit_model_return\u001b[39m\u001b[33m\"\u001b[39m] = ret\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/ml_experiments/ml_experiments/utils.py:131\u001b[39m, in \u001b[36mprofile_time.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    130\u001b[39m start_time = perf_counter()\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m elapsed_time = perf_counter() - start_time\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_in_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mdict\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/ml_experiments/ml_experiments/utils.py:174\u001b[39m, in \u001b[36mprofile_memory.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs_from_func)\u001b[39m\n\u001b[32m    172\u001b[39m     dynamic_enable = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dynamic_enable:\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs_from_func\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m retval:\n\u001b[32m    177\u001b[39m     mem_usage, result = memory_usage((func, args, kwargs_from_func), max_usage=max_usage, retval=retval, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/ml_experiments/ml_experiments/hpo_experiment.py:234\u001b[39m, in \u001b[36mHPOExperiment._fit_model\u001b[39m\u001b[34m(self, combination, unique_params, extra_params, mlflow_run_id, **kwargs)\u001b[39m\n\u001b[32m    222\u001b[39m default_values = \u001b[38;5;28mself\u001b[39m.get_default_values(combination, unique_params, extra_params, mlflow_run_id, **kwargs)\n\u001b[32m    223\u001b[39m get_trial_fn = partial(\n\u001b[32m    224\u001b[39m     \u001b[38;5;28mself\u001b[39m.get_trial_fn,\n\u001b[32m    225\u001b[39m     search_space=search_space,\n\u001b[32m   (...)\u001b[39m\u001b[32m    231\u001b[39m     **kwargs,\n\u001b[32m    232\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m study = \u001b[43mtuner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtune\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m    \u001b[49m\u001b[43msearch_space\u001b[49m\u001b[43m=\u001b[49m\u001b[43msearch_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdirection\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdirection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhpo_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m    \u001b[49m\u001b[43menqueue_configurations\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_trial_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_trial_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcombination\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcombination\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m    \u001b[49m\u001b[43munique_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43munique_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextra_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmlflow_run_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmlflow_run_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m    \u001b[49m\u001b[43mleave_pbar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    249\u001b[39m grid_search_stopped = tuner.grid_search_stopped\n\u001b[32m    250\u001b[39m elapsed_time_timed_out = tuner.timed_out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/ml_experiments/ml_experiments/tuners.py:212\u001b[39m, in \u001b[36mOptunaTuner.tune\u001b[39m\u001b[34m(self, training_fn, search_space, direction, metric, enqueue_configurations, get_trial_fn, hyperband_max_resources, leave_pbar, **kwargs)\u001b[39m\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    210\u001b[39m     max_trials_to_run = \u001b[38;5;28mself\u001b[39m.n_trials - n_trial\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m trials_numbers, results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_trials\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m    \u001b[49m\u001b[43msearch_space\u001b[49m\u001b[43m=\u001b[49m\u001b[43msearch_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_trial_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_trial_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_trials_to_run\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_trials_to_run\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m n_trials_run = \u001b[38;5;28mlen\u001b[39m(trials_numbers)\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m enqueued_configs > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/ml_experiments/ml_experiments/tuners.py:182\u001b[39m, in \u001b[36mOptunaTuner.run_trials\u001b[39m\u001b[34m(self, training_fn, search_space, get_trial_fn, max_trials_to_run, **kwargs)\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_trials\u001b[39m(\u001b[38;5;28mself\u001b[39m, training_fn, search_space, get_trial_fn, max_trials_to_run, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_simple_sequential_trial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtraining_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_space\u001b[49m\u001b[43m=\u001b[49m\u001b[43msearch_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_trial_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_trial_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/ml_experiments/ml_experiments/tuners.py:172\u001b[39m, in \u001b[36mOptunaTuner.run_simple_sequential_trial\u001b[39m\u001b[34m(self, training_fn, search_space, get_trial_fn, **kwargs)\u001b[39m\n\u001b[32m    170\u001b[39m         result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m     result = \u001b[43mtraining_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(trial, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    174\u001b[39m     trial_number = trial[\u001b[33m\"\u001b[39m\u001b[33mtrial\u001b[39m\u001b[33m\"\u001b[39m].number\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/cohirf/cohirf/experiment/hpo_clustering_experiment.py:116\u001b[39m, in \u001b[36mHPOClusteringExperiment.training_fn\u001b[39m\u001b[34m(self, trial_dict, combination, unique_params, extra_params, mlflow_run_id, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m         combination[\u001b[33m\"\u001b[39m\u001b[33mseed_model\u001b[39m\u001b[33m\"\u001b[39m] = seed_model\n\u001b[32m    115\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m mlflow_run_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m             results = \u001b[43msimple_experiment\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_run_mlflow_and_train_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m\t\t\t\t\u001b[49m\u001b[43mcombination\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcombination\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m\t\t\t\t\u001b[49m\u001b[43munique_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43munique_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[43m\t\t\t\t\u001b[49m\u001b[43mextra_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m\t\t\t\t\u001b[49m\u001b[43mmlflow_run_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchild_run_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m\t\t\t\t\u001b[49m\u001b[43mreturn_results\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m\t\t\t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    124\u001b[39m             results = simple_experiment._train_model(\n\u001b[32m    125\u001b[39m \t\t\t\tcombination=combination,\n\u001b[32m    126\u001b[39m \t\t\t\tunique_params=unique_params,\n\u001b[32m   (...)\u001b[39m\u001b[32m    129\u001b[39m \t\t\t\treturn_results=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    130\u001b[39m \t\t\t)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/ml_experiments/ml_experiments/base_experiment.py:1247\u001b[39m, in \u001b[36mBaseExperiment._run_mlflow_and_train_model\u001b[39m\u001b[34m(self, combination, unique_params, extra_params, return_results, mlflow_run_id, **kwargs)\u001b[39m\n\u001b[32m   1237\u001b[39m mlflow_client.update_run(mlflow_run_id, status=\u001b[33m\"\u001b[39m\u001b[33mRUNNING\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1238\u001b[39m \u001b[38;5;28mself\u001b[39m._log_run_start_params(\n\u001b[32m   1239\u001b[39m     combination=combination,\n\u001b[32m   1240\u001b[39m     unique_params=unique_params,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1244\u001b[39m     **kwargs,\n\u001b[32m   1245\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1247\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1248\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcombination\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcombination\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1249\u001b[39m \u001b[43m    \u001b[49m\u001b[43munique_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43munique_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1250\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1251\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextra_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1252\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmlflow_run_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmlflow_run_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1253\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1254\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/ml_experiments/ml_experiments/base_experiment.py:1010\u001b[39m, in \u001b[36mBaseExperiment._train_model\u001b[39m\u001b[34m(self, combination, unique_params, extra_params, return_results, mlflow_run_id, **kwargs)\u001b[39m\n\u001b[32m   1008\u001b[39m         results[\u001b[33m\"\u001b[39m\u001b[33mfit_model_return\u001b[39m\u001b[33m\"\u001b[39m] = ret\n\u001b[32m   1009\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1010\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcombination\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcombination\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43munique_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43munique_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmlflow_run_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmlflow_run_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1018\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ret:\n\u001b[32m   1019\u001b[39m         results[\u001b[33m\"\u001b[39m\u001b[33mfit_model_return\u001b[39m\u001b[33m\"\u001b[39m] = ret\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/ml_experiments/ml_experiments/utils.py:131\u001b[39m, in \u001b[36mprofile_time.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    130\u001b[39m start_time = perf_counter()\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m elapsed_time = perf_counter() - start_time\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_in_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mdict\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/ml_experiments/ml_experiments/utils.py:174\u001b[39m, in \u001b[36mprofile_memory.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs_from_func)\u001b[39m\n\u001b[32m    172\u001b[39m     dynamic_enable = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dynamic_enable:\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs_from_func\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m retval:\n\u001b[32m    177\u001b[39m     mem_usage, result = memory_usage((func, args, kwargs_from_func), max_usage=max_usage, retval=retval, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/cohirf/cohirf/experiment/clustering_experiment.py:253\u001b[39m, in \u001b[36mClusteringExperiment._fit_model\u001b[39m\u001b[34m(self, combination, unique_params, extra_params, mlflow_run_id, **kwargs)\u001b[39m\n\u001b[32m    251\u001b[39m model = kwargs[\u001b[33m'\u001b[39m\u001b[33mload_model_return\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    252\u001b[39m X = kwargs[\u001b[33m'\u001b[39m\u001b[33mload_data_return\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mX\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m y_pred = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m'\u001b[39m\u001b[33my_pred\u001b[39m\u001b[33m'\u001b[39m: y_pred}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/cohirf/cohirf/models/batch_cohirf.py:292\u001b[39m, in \u001b[36mBatchCoHiRF.fit_predict\u001b[39m\u001b[34m(self, X, y, sample_weight, representatives_indexes, parents, labels)\u001b[39m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit_predict\u001b[39m(\n\u001b[32m    284\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    285\u001b[39m     X: pd.DataFrame | np.ndarray,\n\u001b[32m   (...)\u001b[39m\u001b[32m    290\u001b[39m     labels=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    291\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepresentatives_indexes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.automatically_get_labels:\n\u001b[32m    294\u001b[39m         \u001b[38;5;28mself\u001b[39m.get_labels()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/cohirf/cohirf/models/batch_cohirf.py:244\u001b[39m, in \u001b[36mBatchCoHiRF.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, representatives_indexes, parents, labels)\u001b[39m\n\u001b[32m    234\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStarting epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    236\u001b[39m X_representatives = \u001b[38;5;28mself\u001b[39m.get_X_representatives(X_representatives, representatives_local_indexes)\n\u001b[32m    238\u001b[39m (\n\u001b[32m    239\u001b[39m     new_representatives_local_indexes,\n\u001b[32m    240\u001b[39m     new_local_parents,\n\u001b[32m    241\u001b[39m     new_local_labels,\n\u001b[32m    242\u001b[39m     new_n_clusters,\n\u001b[32m    243\u001b[39m     stop,\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_representatives\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.hierarchy_strategy == \u001b[33m\"\u001b[39m\u001b[33mparents\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    247\u001b[39m     new_absolute_parents = representatives_absolute_indexes[new_local_parents]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/cohirf/cohirf/models/batch_cohirf.py:87\u001b[39m, in \u001b[36mBatchCoHiRF.run_one_epoch\u001b[39m\u001b[34m(self, X_representatives)\u001b[39m\n\u001b[32m     84\u001b[39m     last_epoch = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     86\u001b[39m parallel = Parallel(n_jobs=\u001b[38;5;28mself\u001b[39m.n_jobs, return_as=\u001b[33m\"\u001b[39m\u001b[33mlist\u001b[39m\u001b[33m\"\u001b[39m, verbose=\u001b[38;5;28mself\u001b[39m.verbose)\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m results = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_one_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_representatives\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_batches\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m all_parents, all_labels, all_representatives_indexes, all_n_clusters = \u001b[38;5;28mzip\u001b[39m(*results)\n\u001b[32m     89\u001b[39m all_parents = \u001b[38;5;28mlist\u001b[39m(all_parents)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cohirf/lib/python3.11/site-packages/joblib/parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cohirf/lib/python3.11/site-packages/joblib/parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cohirf/lib/python3.11/site-packages/joblib/parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         time.sleep(\u001b[32m0.01\u001b[39m)\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "hpo_metric = \"adjusted_mutual_info\"\n",
    "experiment = HPOCSVClusteringExperiment(\n",
    "    # hpo\n",
    "    n_trials=20,\n",
    "    hpo_seed=0,\n",
    "    hpo_metric=hpo_metric,\n",
    "    direction=\"maximize\",\n",
    "    # model\n",
    "    experiment_name=\"test-spotify\",\n",
    "    model=\"BatchCoHiRF-DBSCAN-1iter\",\n",
    "    model_params=dict(n_batches=100),\n",
    "    n_jobs=10,\n",
    "    seed_model=0,\n",
    "    # dataset\n",
    "    dataset_name=\"spotify\",\n",
    "    raise_on_error=True,\n",
    "    standardize=True,\n",
    "    verbose=1,\n",
    "    **experiment_params,\n",
    ")\n",
    "result = experiment.run(return_results=True)[0]\n",
    "metric = result[\"evaluate_model_return\"][f\"best/{hpo_metric}\"]\n",
    "hpo_time = result[\"fit_model_return\"][\"elapsed_time\"]\n",
    "best_time = result[\"evaluate_model_return\"][\"best/elapsed_time\"]\n",
    "print(f\"{hpo_metric}: {metric:.4f}, HPO time: {hpo_time:.2f}s, Best time: {best_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "229c0a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARI: 0.1013, HPO time: 355.74s, Best time: 1.18s\n"
     ]
    }
   ],
   "source": [
    "print(f\"ARI: {ari:.4f}, HPO time: {hpo_time:.2f}s, Best time: {best_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5929976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'work_dir': PosixPath('/home/belucci/code/cohirf/development_notebooks/work/ac67daf48c144225ab4d08af4db788eb'),\n",
       " 'save_dir': None,\n",
       " 'load_data_return': {'X':         popularity  key_mode  danceability    energy  loudness  speechiness  \\\n",
       "  0         1.932553 -1.224541      0.648384 -0.682903  0.338832     0.479251   \n",
       "  1         1.025878 -1.196378     -0.799022 -1.830501 -1.663264    -0.103467   \n",
       "  2         1.126620 -1.478010     -0.697251 -1.079700 -0.231505    -0.283437   \n",
       "  3         1.831812 -1.478010     -1.669727 -2.244414 -1.907585    -0.452923   \n",
       "  4         2.385891 -0.914745      0.320456 -0.752926 -0.221389    -0.310520   \n",
       "  ...            ...       ...           ...       ...       ...          ...   \n",
       "  113994   -0.686731 -0.069849     -2.201197 -1.562080 -1.502547    -0.401378   \n",
       "  113995   -0.636360 -1.506173     -2.189889 -2.021119 -1.869983    -0.419725   \n",
       "  113996   -0.636360 -1.506173      0.382649 -1.196404 -0.453112    -0.403126   \n",
       "  113997    0.320686  0.493416      0.145184 -0.507846 -0.451966    -0.510584   \n",
       "  113998   -0.636360 -1.224541     -0.199705 -0.581759 -0.321217    -0.136665   \n",
       "  \n",
       "          acousticness  instrumentalness  liveness   valence     tempo  \n",
       "  0          -0.869279         -0.545788  0.709881  0.944714 -1.140295  \n",
       "  1           1.765787         -0.545774 -0.597345 -0.759220 -1.487031  \n",
       "  2          -0.343920         -0.545791 -0.515961 -1.318323 -1.525502  \n",
       "  3           1.709647         -0.545575 -0.439664 -1.230844  1.979362  \n",
       "  4           0.421366         -0.545791 -0.689410 -1.139562 -0.075217  \n",
       "  ...              ...               ...       ...       ...       ...  \n",
       "  113994      0.926632          2.289426 -0.672116 -1.645798  0.125816  \n",
       "  113995      1.972621          2.436075 -0.576999 -1.641614 -1.229340  \n",
       "  113996      1.597365         -0.545791 -0.684323  1.051210  0.338053  \n",
       "  113997      0.161346         -0.545791  0.262270 -0.203920  0.457156  \n",
       "  113998      1.047778         -0.545791 -0.656856  0.918090 -1.430206  \n",
       "  \n",
       "  [86701 rows x 11 columns],\n",
       "  'y': 0            acoustic\n",
       "  1            acoustic\n",
       "  2            acoustic\n",
       "  3            acoustic\n",
       "  4            acoustic\n",
       "               ...     \n",
       "  113994    world-music\n",
       "  113995    world-music\n",
       "  113996    world-music\n",
       "  113997    world-music\n",
       "  113998    world-music\n",
       "  Name: track_genre, Length: 86701, dtype: object,\n",
       "  'cat_features_names': [],\n",
       "  'n_classes': 114,\n",
       "  'dataset_name': 'spotify'},\n",
       " 'load_model_return': {'tuner': <ml_experiments.tuners.OptunaTuner at 0x79660a311210>},\n",
       " 'before_fit_model_return': {'simple_experiment': <cohirf.experiment.csv_clustering_experiment.CSVClusteringExperiment at 0x796661efbc10>,\n",
       "  'random_generator': Generator(PCG64) at 0x7965FCA2ACE0},\n",
       " 'max_memory_used_before_fit': 456.944,\n",
       " 'fit_model_return': {'study': <optuna.study.study.Study at 0x7965f56d6750>,\n",
       "  'elapsed_time': 669.2687572139985},\n",
       " 'max_memory_used_after_fit': 456.944,\n",
       " 'evaluate_model_return': {'best/n_clusters_': 9871,\n",
       "  'best/rand_score': 0.9929571975392714,\n",
       "  'best/adjusted_rand': 0.5941062806437477,\n",
       "  'best/mutual_info': 4.171445599059551,\n",
       "  'best/adjusted_mutual_info': 0.7887996324563251,\n",
       "  'best/normalized_mutual_info': 0.814780875408223,\n",
       "  'best/homogeneity': 0.8932090370992277,\n",
       "  'best/completeness': 0.749013769827233,\n",
       "  'best/v_measure': 0.814780875408223,\n",
       "  'best/silhouette': -0.34535523824272785,\n",
       "  'best/calinski_harabasz_score': 7.62074547910128,\n",
       "  'best/elapsed_time': 32.97532700699958,\n",
       "  'best/value': 0.5941062806437477},\n",
       " 'total_elapsed_time': 669.7639271970002,\n",
       " 'combination': {'model': 'BatchCoHiRF-DBSCAN-1iter',\n",
       "  'seed_model': 0,\n",
       "  'dataset_name': 'spotify'},\n",
       " 'unique_params': {'timeout_fit': None,\n",
       "  'timeout_combination': None,\n",
       "  'n_jobs': 10,\n",
       "  'model_params': {'n_batches': 100,\n",
       "   'cohirf_kwargs': {'n_features': 0.6636476857268381,\n",
       "    'repetitions': 9,\n",
       "    'base_model_kwargs': {'eps': 0.7178158451945069, 'min_samples': 30}}},\n",
       "  'max_threads': None,\n",
       "  'calculate_davies_bouldin': False,\n",
       "  'chunk_size_davies_bouldin': None,\n",
       "  'calculate_full_silhouette': False,\n",
       "  'calculate_metrics_even_if_too_many_clusters': False,\n",
       "  'standardize': True,\n",
       "  'hpo_framework': 'optuna',\n",
       "  'n_trials': 20,\n",
       "  'timeout_hpo': 0,\n",
       "  'timeout_trial': 0,\n",
       "  'max_concurrent_trials': 1,\n",
       "  'hpo_seed': 0,\n",
       "  'sampler': 'tpe',\n",
       "  'pruner': 'none',\n",
       "  'direction': 'maximize',\n",
       "  'hpo_metric': 'adjusted_rand'},\n",
       " 'extra_params': {},\n",
       " 'mlflow_run_id': 'ac67daf48c144225ab4d08af4db788eb',\n",
       " 'Finished': True}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90ff69d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dccb74c368e490a8d9bce168916f06b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Combinations completed:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7259af825c0748adb7450920769f3c70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Trials:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARI: 0.0274, HPO time: 127.72s, Best time: 6.09s\n"
     ]
    }
   ],
   "source": [
    "experiment = HPOCSVClusteringExperiment(\n",
    "    # hpo\n",
    "    n_trials=20,\n",
    "    hpo_seed=0,\n",
    "    hpo_metric=\"adjusted_rand\",\n",
    "    direction=\"maximize\",\n",
    "    # model\n",
    "    experiment_name=\"test-spotify\",\n",
    "    model=\"CoHiRF\",\n",
    "\tmodel_params=dict(n_samples_representative=10_000),\n",
    "    seed_model=0,\n",
    "    # dataset\n",
    "    dataset_name=\"spotify\",\n",
    "    raise_on_error=True,\n",
    "    standardize=False,\n",
    "    verbose=1,\n",
    "    **experiment_params,\n",
    ")\n",
    "result = experiment.run(return_results=True)[0]\n",
    "ari = result[\"evaluate_model_return\"][\"best/adjusted_rand\"]\n",
    "hpo_time = result[\"fit_model_return\"][\"elapsed_time\"]\n",
    "best_time = result[\"evaluate_model_return\"][\"best/elapsed_time\"]\n",
    "print(f\"ARI: {ari:.4f}, HPO time: {hpo_time:.2f}s, Best time: {best_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98494e99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "947330332b314061a372af9706aaf77f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Combinations completed:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "987d240bfcad446fb5eeb96c2c747c06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Trials:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARI: 0.0629, HPO time: 239.93s, Best time: 10.98s\n"
     ]
    }
   ],
   "source": [
    "experiment = HPOCSVClusteringExperiment(\n",
    "    # hpo\n",
    "    n_trials=20,\n",
    "    hpo_seed=0,\n",
    "    hpo_metric=\"adjusted_rand\",\n",
    "    direction=\"maximize\",\n",
    "    # model\n",
    "    experiment_name=\"test-spotify\",\n",
    "    model=\"BatchCoHiRF-1iter\",\n",
    "    # model_params=dict(n_samples_representative=10_000),\n",
    "    seed_model=0,\n",
    "    # dataset\n",
    "    dataset_name=\"spotify\",\n",
    "    raise_on_error=True,\n",
    "    standardize=False,\n",
    "    verbose=1,\n",
    "    **experiment_params,\n",
    ")\n",
    "result = experiment.run(return_results=True)[0]\n",
    "ari = result[\"evaluate_model_return\"][\"best/adjusted_rand\"]\n",
    "hpo_time = result[\"fit_model_return\"][\"elapsed_time\"]\n",
    "best_time = result[\"evaluate_model_return\"][\"best/elapsed_time\"]\n",
    "print(f\"ARI: {ari:.4f}, HPO time: {hpo_time:.2f}s, Best time: {best_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8e740e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "596e604712b64f8fbbe3cb473f0e40e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Combinations completed:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d2606ff716b42f494a2b14cde481c64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Trials:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARI: 0.0587, HPO time: 570.64s, Best time: 40.49s\n"
     ]
    }
   ],
   "source": [
    "experiment = HPOCSVClusteringExperiment(\n",
    "    # hpo\n",
    "    n_trials=20,\n",
    "    hpo_seed=0,\n",
    "    hpo_metric=\"adjusted_rand\",\n",
    "    direction=\"maximize\",\n",
    "    # model\n",
    "    experiment_name=\"test-spotify\",\n",
    "    model=\"BatchCoHiRF-KernelRBF-1iter\",\n",
    "\tn_jobs=10,\n",
    "    # model_params=dict(n_samples_representative=10_000),\n",
    "    seed_model=0,\n",
    "    # dataset\n",
    "    dataset_name=\"spotify\",\n",
    "    raise_on_error=True,\n",
    "    standardize=False,\n",
    "    verbose=1,\n",
    "    **experiment_params,\n",
    ")\n",
    "result = experiment.run(return_results=True)[0]\n",
    "ari = result[\"evaluate_model_return\"][\"best/adjusted_rand\"]\n",
    "hpo_time = result[\"fit_model_return\"][\"elapsed_time\"]\n",
    "best_time = result[\"evaluate_model_return\"][\"best/elapsed_time\"]\n",
    "print(f\"ARI: {ari:.4f}, HPO time: {hpo_time:.2f}s, Best time: {best_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a64c40",
   "metadata": {},
   "source": [
    "# CRITEO-UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68f80fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c2f81c285f94d58ba3f24d36308d54d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Combinations completed:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7890c760820343118732953af91e81a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Trials:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARI: 0.2207, HPO time: 181.64s, Best time: 1.11s\n"
     ]
    }
   ],
   "source": [
    "experiment = HPOCSVClusteringExperiment(\n",
    "    # hpo\n",
    "    n_trials=20,\n",
    "    hpo_seed=0,\n",
    "    hpo_metric=\"adjusted_rand\",\n",
    "    direction=\"maximize\",\n",
    "    # model\n",
    "    experiment_name=\"test-criteo-bal\",\n",
    "    model=\"KMeans\",\n",
    "    seed_model=0,\n",
    "    # dataset\n",
    "    dataset_name=\"criteo-up-bal\", \n",
    "\traise_on_error=True,\n",
    "\tstandardize=True,\n",
    "\tverbose=1,\n",
    "    **experiment_params,\n",
    ")\n",
    "result = experiment.run(return_results=True)[0]\n",
    "ari = result[\"evaluate_model_return\"][\"best/adjusted_rand\"]\n",
    "hpo_time = result[\"fit_model_return\"][\"elapsed_time\"]\n",
    "best_time = result[\"evaluate_model_return\"][\"best/elapsed_time\"]\n",
    "print(f\"ARI: {ari:.4f}, HPO time: {hpo_time:.2f}s, Best time: {best_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8aee7519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8be818f4540047fdbe5eae1fd8e86d1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Combinations completed:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/20 13:05:47 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2025/08/20 13:05:47 INFO mlflow.store.db.utils: Updating database tables\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7ec17bb9c90423998b59084f31bb19e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Trials:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/belucci/miniconda3/envs/cohirf/lib/python3.11/site-packages/sklearn/base.py:1363: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/belucci/miniconda3/envs/cohirf/lib/python3.11/site-packages/sklearn/base.py:1363: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/belucci/miniconda3/envs/cohirf/lib/python3.11/site-packages/sklearn/base.py:1363: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/belucci/miniconda3/envs/cohirf/lib/python3.11/site-packages/sklearn/base.py:1363: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/belucci/miniconda3/envs/cohirf/lib/python3.11/site-packages/sklearn/base.py:1363: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/belucci/miniconda3/envs/cohirf/lib/python3.11/site-packages/sklearn/base.py:1363: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/belucci/miniconda3/envs/cohirf/lib/python3.11/site-packages/sklearn/base.py:1363: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/belucci/miniconda3/envs/cohirf/lib/python3.11/site-packages/sklearn/base.py:1363: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/belucci/miniconda3/envs/cohirf/lib/python3.11/site-packages/sklearn/base.py:1363: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/belucci/miniconda3/envs/cohirf/lib/python3.11/site-packages/sklearn/base.py:1363: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/belucci/miniconda3/envs/cohirf/lib/python3.11/site-packages/sklearn/base.py:1363: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/belucci/miniconda3/envs/cohirf/lib/python3.11/site-packages/sklearn/base.py:1363: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/belucci/miniconda3/envs/cohirf/lib/python3.11/site-packages/sklearn/base.py:1363: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/belucci/miniconda3/envs/cohirf/lib/python3.11/site-packages/sklearn/base.py:1363: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/belucci/miniconda3/envs/cohirf/lib/python3.11/site-packages/sklearn/base.py:1363: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/belucci/miniconda3/envs/cohirf/lib/python3.11/site-packages/sklearn/base.py:1363: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARI: 0.2914, HPO time: 621.12s, Best time: 7.87s\n"
     ]
    }
   ],
   "source": [
    "experiment = HPOCSVClusteringExperiment(\n",
    "    # hpo\n",
    "    n_trials=20,\n",
    "    hpo_seed=0,\n",
    "    hpo_metric=\"adjusted_rand\",\n",
    "    direction=\"maximize\",\n",
    "    # model\n",
    "    experiment_name=\"test-criteo-bal\",\n",
    "    model=\"BatchCoHiRF-1iter\",\n",
    "    model_params=dict(cohirf_kwargs=dict(n_samples_representative=5000)),\n",
    "    n_jobs=10,\n",
    "    seed_model=0,\n",
    "    # dataset\n",
    "    dataset_name=\"criteo-up-bal\",\n",
    "    raise_on_error=True,\n",
    "    standardize=True,\n",
    "    verbose=1,\n",
    "    **experiment_params,\n",
    ")\n",
    "result = experiment.run(return_results=True)[0]\n",
    "ari = result[\"evaluate_model_return\"][\"best/adjusted_rand\"]\n",
    "hpo_time = result[\"fit_model_return\"][\"elapsed_time\"]\n",
    "best_time = result[\"evaluate_model_return\"][\"best/elapsed_time\"]\n",
    "print(f\"ARI: {ari:.4f}, HPO time: {hpo_time:.2f}s, Best time: {best_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f929b3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 13:49:39\n",
      "Starting experiment...\n",
      "combination_names: ['model', 'seed_model', 'dataset_name']\n",
      "combinations: [('BatchCoHiRF-1iter', 0, 'criteo-up-sub')]\n",
      "unique_params: {'timeout_fit': None, 'timeout_combination': None, 'n_jobs': 10, 'model_params': {'cohirf_kwargs': {'n_samples_representative': 10000}}, 'max_threads': None, 'calculate_davies_bouldin': False, 'calculate_full_silhouette': False, 'calculate_metrics_even_if_too_many_clusters': False, 'standardize': False, 'hpo_framework': 'optuna', 'n_trials': 20, 'timeout_hpo': 0, 'timeout_trial': 0, 'max_concurrent_trials': 1, 'hpo_seed': 0, 'sampler': 'tpe', 'pruner': 'none', 'direction': 'maximize', 'hpo_metric': 'adjusted_rand'}\n",
      "extra_params: {}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7a010a0105340ba927d62d7eda2ad09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Combinations completed:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 13:49:40\n",
      "Running...\n",
      "model: BatchCoHiRF-1iter\n",
      "seed_model: 0\n",
      "dataset_name: criteo-up-sub\n",
      "timeout_fit: None\n",
      "timeout_combination: None\n",
      "n_jobs: 10\n",
      "model_params: {'cohirf_kwargs': {'n_samples_representative': 10000}}\n",
      "max_threads: None\n",
      "calculate_davies_bouldin: False\n",
      "calculate_full_silhouette: False\n",
      "calculate_metrics_even_if_too_many_clusters: False\n",
      "standardize: False\n",
      "hpo_framework: optuna\n",
      "n_trials: 20\n",
      "timeout_hpo: 0\n",
      "timeout_trial: 0\n",
      "max_concurrent_trials: 1\n",
      "hpo_seed: 0\n",
      "sampler: tpe\n",
      "pruner: none\n",
      "direction: maximize\n",
      "hpo_metric: adjusted_rand\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e10d0c28cb2e41db9fd19054c995e5f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Trials:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/belucci/miniconda3/envs/cohirf/lib/python3.11/site-packages/sklearn/base.py:1363: ConvergenceWarning: Number of distinct clusters (4) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/belucci/miniconda3/envs/cohirf/lib/python3.11/site-packages/sklearn/base.py:1363: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/belucci/miniconda3/envs/cohirf/lib/python3.11/site-packages/sklearn/base.py:1363: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/belucci/miniconda3/envs/cohirf/lib/python3.11/site-packages/sklearn/base.py:1363: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/belucci/miniconda3/envs/cohirf/lib/python3.11/site-packages/sklearn/base.py:1363: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/belucci/miniconda3/envs/cohirf/lib/python3.11/site-packages/sklearn/base.py:1363: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/belucci/miniconda3/envs/cohirf/lib/python3.11/site-packages/sklearn/base.py:1363: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/belucci/miniconda3/envs/cohirf/lib/python3.11/site-packages/sklearn/base.py:1363: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/belucci/miniconda3/envs/cohirf/lib/python3.11/site-packages/sklearn/base.py:1363: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/belucci/miniconda3/envs/cohirf/lib/python3.11/site-packages/sklearn/base.py:1363: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/belucci/miniconda3/envs/cohirf/lib/python3.11/site-packages/sklearn/base.py:1363: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/belucci/miniconda3/envs/cohirf/lib/python3.11/site-packages/sklearn/base.py:1363: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/belucci/miniconda3/envs/cohirf/lib/python3.11/site-packages/sklearn/base.py:1363: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/belucci/miniconda3/envs/cohirf/lib/python3.11/site-packages/sklearn/base.py:1363: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/belucci/miniconda3/envs/cohirf/lib/python3.11/site-packages/sklearn/base.py:1363: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/belucci/miniconda3/envs/cohirf/lib/python3.11/site-packages/sklearn/base.py:1363: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/belucci/miniconda3/envs/cohirf/lib/python3.11/site-packages/sklearn/base.py:1363: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/belucci/miniconda3/envs/cohirf/lib/python3.11/site-packages/sklearn/base.py:1363: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/belucci/miniconda3/envs/cohirf/lib/python3.11/site-packages/sklearn/base.py:1363: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/belucci/miniconda3/envs/cohirf/lib/python3.11/site-packages/sklearn/base.py:1363: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/belucci/miniconda3/envs/cohirf/lib/python3.11/site-packages/sklearn/base.py:1363: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/belucci/miniconda3/envs/cohirf/lib/python3.11/site-packages/sklearn/base.py:1363: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/belucci/miniconda3/envs/cohirf/lib/python3.11/site-packages/sklearn/base.py:1363: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/belucci/miniconda3/envs/cohirf/lib/python3.11/site-packages/sklearn/base.py:1363: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/belucci/miniconda3/envs/cohirf/lib/python3.11/site-packages/sklearn/base.py:1363: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 13:50:26\n",
      "Finished!\n",
      "total_elapsed_time: 49.326898499999515\n",
      "model: BatchCoHiRF-1iter\n",
      "seed_model: 0\n",
      "dataset_name: criteo-up-sub\n",
      "timeout_fit: None\n",
      "timeout_combination: None\n",
      "n_jobs: 10\n",
      "model_params: {'cohirf_kwargs': {'n_samples_representative': 10000, 'n_features': 0.30288715291158325, 'repetitions': 6, 'kmeans_n_clusters': 3}}\n",
      "max_threads: None\n",
      "calculate_davies_bouldin: False\n",
      "calculate_full_silhouette: False\n",
      "calculate_metrics_even_if_too_many_clusters: False\n",
      "standardize: False\n",
      "hpo_framework: optuna\n",
      "n_trials: 20\n",
      "timeout_hpo: 0\n",
      "timeout_trial: 0\n",
      "max_concurrent_trials: 1\n",
      "hpo_seed: 0\n",
      "sampler: tpe\n",
      "pruner: none\n",
      "direction: maximize\n",
      "hpo_metric: adjusted_rand\n",
      "\n",
      "2025-08-11 13:50:26\n",
      "Combinations completed:   0%|          | 0/1 [00:47<?, ?it/s]\n",
      "succesfully_completed: 1\n",
      "failed: 0\n",
      "none: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiment = HPOCSVClusteringExperiment(\n",
    "    # hpo\n",
    "    n_trials=20,\n",
    "    hpo_seed=0,\n",
    "    hpo_metric=\"adjusted_rand\",\n",
    "    direction=\"maximize\",\n",
    "    # model\n",
    "    experiment_name=\"test-criteo-sub\",\n",
    "    model=\"BatchCoHiRF-1iter\",\n",
    "    seed_model=0,\n",
    "\tmodel_params=dict(cohirf_kwargs=dict(n_samples_representative=10000)),\n",
    "\tn_jobs=10,\n",
    "    # dataset\n",
    "    dataset_name=\"criteo-up-sub\",\n",
    "    # raise_on_error=True,\n",
    "    **experiment_params,\n",
    ")\n",
    "result = experiment.run(return_results=True)[0]\n",
    "ari = result[\"evaluate_model_return\"][\"best/adjusted_rand\"]\n",
    "hpo_time = result[\"fit_model_return\"][\"elapsed_time\"]\n",
    "best_time = result[\"evaluate_model_return\"][\"best/elapsed_time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f1d45fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARI: 0.0014, HPO time: 48.98s, Best time: 2.20s\n"
     ]
    }
   ],
   "source": [
    "print(f\"ARI: {ari:.4f}, HPO time: {hpo_time:.2f}s, Best time: {best_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e368f1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 13:52:25\n",
      "Starting experiment...\n",
      "combination_names: ['model', 'seed_model', 'dataset_name']\n",
      "combinations: [('BatchCoHiRF-DBSCAN-1iter', 0, 'criteo-up-sub')]\n",
      "unique_params: {'timeout_fit': None, 'timeout_combination': None, 'n_jobs': 5, 'model_params': {'cohirf_kwargs': {'n_samples_representative': 10000}, 'n_batches': 10}, 'max_threads': None, 'calculate_davies_bouldin': False, 'calculate_full_silhouette': False, 'calculate_metrics_even_if_too_many_clusters': False, 'standardize': False, 'hpo_framework': 'optuna', 'n_trials': 20, 'timeout_hpo': 0, 'timeout_trial': 0, 'max_concurrent_trials': 1, 'hpo_seed': 0, 'sampler': 'tpe', 'pruner': 'none', 'direction': 'maximize', 'hpo_metric': 'adjusted_rand'}\n",
      "extra_params: {}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d53462762b3e4e708a21659cd17db7b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Combinations completed:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 13:52:26\n",
      "Running...\n",
      "model: BatchCoHiRF-DBSCAN-1iter\n",
      "seed_model: 0\n",
      "dataset_name: criteo-up-sub\n",
      "timeout_fit: None\n",
      "timeout_combination: None\n",
      "n_jobs: 5\n",
      "model_params: {'cohirf_kwargs': {'n_samples_representative': 10000}, 'n_batches': 10}\n",
      "max_threads: None\n",
      "calculate_davies_bouldin: False\n",
      "calculate_full_silhouette: False\n",
      "calculate_metrics_even_if_too_many_clusters: False\n",
      "standardize: False\n",
      "hpo_framework: optuna\n",
      "n_trials: 20\n",
      "timeout_hpo: 0\n",
      "timeout_trial: 0\n",
      "max_concurrent_trials: 1\n",
      "hpo_seed: 0\n",
      "sampler: tpe\n",
      "pruner: none\n",
      "direction: maximize\n",
      "hpo_metric: adjusted_rand\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54b02b66817c4313afecae9e2489d114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Trials:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/belucci/miniconda3/envs/cohirf/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "/home/belucci/miniconda3/envs/cohirf/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 14:00:58\n",
      "Finished!\n",
      "total_elapsed_time: 564.4211306799989\n",
      "model: BatchCoHiRF-DBSCAN-1iter\n",
      "seed_model: 0\n",
      "dataset_name: criteo-up-sub\n",
      "timeout_fit: None\n",
      "timeout_combination: None\n",
      "n_jobs: 5\n",
      "model_params: {'cohirf_kwargs': {'n_samples_representative': 10000, 'n_features': 0.44509329437166845, 'repetitions': 4, 'base_model_kwargs': {'eps': 8.613865017344951, 'min_samples': 5}}, 'n_batches': 10}\n",
      "max_threads: None\n",
      "calculate_davies_bouldin: False\n",
      "calculate_full_silhouette: False\n",
      "calculate_metrics_even_if_too_many_clusters: False\n",
      "standardize: False\n",
      "hpo_framework: optuna\n",
      "n_trials: 20\n",
      "timeout_hpo: 0\n",
      "timeout_trial: 0\n",
      "max_concurrent_trials: 1\n",
      "hpo_seed: 0\n",
      "sampler: tpe\n",
      "pruner: none\n",
      "direction: maximize\n",
      "hpo_metric: adjusted_rand\n",
      "\n",
      "2025-08-11 14:00:58\n",
      "Combinations completed:   0%|          | 0/1 [08:32<?, ?it/s]\n",
      "succesfully_completed: 1\n",
      "failed: 0\n",
      "none: 0\n",
      "\n",
      "ARI: 0.0208, HPO time: 563.70s, Best time: 20.49s\n"
     ]
    }
   ],
   "source": [
    "experiment = HPOCSVClusteringExperiment(\n",
    "    # hpo\n",
    "    n_trials=20,\n",
    "    hpo_seed=0,\n",
    "    hpo_metric=\"adjusted_rand\",\n",
    "    direction=\"maximize\",\n",
    "    # model\n",
    "    experiment_name=\"test-criteo-sub\",\n",
    "    model=\"BatchCoHiRF-DBSCAN-1iter\",\n",
    "    seed_model=0,\n",
    "    model_params=dict(cohirf_kwargs=dict(n_samples_representative=10000), n_batches=10),\n",
    "    n_jobs=5,\n",
    "    # dataset\n",
    "    dataset_name=\"criteo-up-sub\",\n",
    "    raise_on_error=True,\n",
    "    **experiment_params,\n",
    ")\n",
    "result = experiment.run(return_results=True)[0]\n",
    "ari = result[\"evaluate_model_return\"][\"best/adjusted_rand\"]\n",
    "hpo_time = result[\"fit_model_return\"][\"elapsed_time\"]\n",
    "best_time = result[\"evaluate_model_return\"][\"best/elapsed_time\"]\n",
    "print(f\"ARI: {ari:.4f}, HPO time: {hpo_time:.2f}s, Best time: {best_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccd1fa2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 14:01:59\n",
      "Starting experiment...\n",
      "combination_names: ['model', 'seed_model', 'dataset_name']\n",
      "combinations: [('BatchCoHiRF-KernelRBF-1iter', 0, 'criteo-up-sub')]\n",
      "unique_params: {'timeout_fit': None, 'timeout_combination': None, 'n_jobs': 10, 'model_params': {'cohirf_kwargs': {'n_samples_representative': 10000}, 'n_batches': 10}, 'max_threads': None, 'calculate_davies_bouldin': False, 'calculate_full_silhouette': False, 'calculate_metrics_even_if_too_many_clusters': False, 'standardize': False, 'hpo_framework': 'optuna', 'n_trials': 20, 'timeout_hpo': 0, 'timeout_trial': 0, 'max_concurrent_trials': 1, 'hpo_seed': 0, 'sampler': 'tpe', 'pruner': 'none', 'direction': 'maximize', 'hpo_metric': 'adjusted_rand'}\n",
      "extra_params: {}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45a0da4959e64905b4ae949ff7f86c6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Combinations completed:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 14:02:00\n",
      "Running...\n",
      "model: BatchCoHiRF-KernelRBF-1iter\n",
      "seed_model: 0\n",
      "dataset_name: criteo-up-sub\n",
      "timeout_fit: None\n",
      "timeout_combination: None\n",
      "n_jobs: 10\n",
      "model_params: {'cohirf_kwargs': {'n_samples_representative': 10000}, 'n_batches': 10}\n",
      "max_threads: None\n",
      "calculate_davies_bouldin: False\n",
      "calculate_full_silhouette: False\n",
      "calculate_metrics_even_if_too_many_clusters: False\n",
      "standardize: False\n",
      "hpo_framework: optuna\n",
      "n_trials: 20\n",
      "timeout_hpo: 0\n",
      "timeout_trial: 0\n",
      "max_concurrent_trials: 1\n",
      "hpo_seed: 0\n",
      "sampler: tpe\n",
      "pruner: none\n",
      "direction: maximize\n",
      "hpo_metric: adjusted_rand\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e83c33403f0461bbe30fd3d61d38714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Trials:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/belucci/miniconda3/envs/cohirf/lib/python3.11/site-packages/sklearn/base.py:1363: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (5). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/belucci/miniconda3/envs/cohirf/lib/python3.11/site-packages/sklearn/base.py:1363: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (5). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/belucci/miniconda3/envs/cohirf/lib/python3.11/site-packages/sklearn/base.py:1363: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (5). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/belucci/miniconda3/envs/cohirf/lib/python3.11/site-packages/sklearn/base.py:1363: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (5). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/belucci/miniconda3/envs/cohirf/lib/python3.11/site-packages/sklearn/base.py:1363: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (5). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/belucci/miniconda3/envs/cohirf/lib/python3.11/site-packages/sklearn/base.py:1363: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (5). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/belucci/miniconda3/envs/cohirf/lib/python3.11/site-packages/sklearn/base.py:1363: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/belucci/miniconda3/envs/cohirf/lib/python3.11/site-packages/sklearn/base.py:1363: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/belucci/miniconda3/envs/cohirf/lib/python3.11/site-packages/sklearn/base.py:1363: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/belucci/miniconda3/envs/cohirf/lib/python3.11/site-packages/sklearn/base.py:1363: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/belucci/miniconda3/envs/cohirf/lib/python3.11/site-packages/sklearn/base.py:1363: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 14:04:59\n",
      "Finished!\n",
      "total_elapsed_time: 196.0536806580003\n",
      "model: BatchCoHiRF-KernelRBF-1iter\n",
      "seed_model: 0\n",
      "dataset_name: criteo-up-sub\n",
      "timeout_fit: None\n",
      "timeout_combination: None\n",
      "n_jobs: 10\n",
      "model_params: {'cohirf_kwargs': {'n_samples_representative': 10000, 'n_features': 0.29822878547828213, 'repetitions': 1, 'base_model_kwargs': {'n_clusters': 2}, 'transform_kwargs': {'gamma': 25.806303962205945}}, 'n_batches': 10}\n",
      "max_threads: None\n",
      "calculate_davies_bouldin: False\n",
      "calculate_full_silhouette: False\n",
      "calculate_metrics_even_if_too_many_clusters: False\n",
      "standardize: False\n",
      "hpo_framework: optuna\n",
      "n_trials: 20\n",
      "timeout_hpo: 0\n",
      "timeout_trial: 0\n",
      "max_concurrent_trials: 1\n",
      "hpo_seed: 0\n",
      "sampler: tpe\n",
      "pruner: none\n",
      "direction: maximize\n",
      "hpo_metric: adjusted_rand\n",
      "\n",
      "2025-08-11 14:04:59\n",
      "Combinations completed:   0%|          | 0/1 [02:59<?, ?it/s]\n",
      "succesfully_completed: 1\n",
      "failed: 0\n",
      "none: 0\n",
      "\n",
      "ARI: 0.0158, HPO time: 195.49s, Best time: 8.85s\n"
     ]
    }
   ],
   "source": [
    "experiment = HPOCSVClusteringExperiment(\n",
    "    # hpo\n",
    "    n_trials=20,\n",
    "    hpo_seed=0,\n",
    "    hpo_metric=\"adjusted_rand\",\n",
    "    direction=\"maximize\",\n",
    "    # model\n",
    "    experiment_name=\"test-criteo-sub\",\n",
    "    model=\"BatchCoHiRF-KernelRBF-1iter\",\n",
    "    seed_model=0,\n",
    "    model_params=dict(cohirf_kwargs=dict(n_samples_representative=10000), n_batches=10),\n",
    "    n_jobs=10,\n",
    "    # dataset\n",
    "    dataset_name=\"criteo-up-sub\",\n",
    "    raise_on_error=True,\n",
    "    **experiment_params,\n",
    ")\n",
    "result = experiment.run(return_results=True)[0]\n",
    "ari = result[\"evaluate_model_return\"][\"best/adjusted_rand\"]\n",
    "hpo_time = result[\"fit_model_return\"][\"elapsed_time\"]\n",
    "best_time = result[\"evaluate_model_return\"][\"best/elapsed_time\"]\n",
    "print(f\"ARI: {ari:.4f}, HPO time: {hpo_time:.2f}s, Best time: {best_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "793a9e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 14:17:29\n",
      "Starting experiment...\n",
      "combination_names: ['model', 'seed_model', 'dataset_name']\n",
      "combinations: [('BatchCoHiRF-SC-SRGF', 0, 'criteo-up-sub')]\n",
      "unique_params: {'timeout_fit': None, 'timeout_combination': None, 'n_jobs': 10, 'model_params': {'cohirf_kwargs': {'n_samples_representative': 10000}, 'n_batches': 100}, 'max_threads': None, 'calculate_davies_bouldin': False, 'calculate_full_silhouette': False, 'calculate_metrics_even_if_too_many_clusters': False, 'standardize': False, 'hpo_framework': 'optuna', 'n_trials': 20, 'timeout_hpo': 0, 'timeout_trial': 0, 'max_concurrent_trials': 1, 'hpo_seed': 0, 'sampler': 'tpe', 'pruner': 'none', 'direction': 'maximize', 'hpo_metric': 'adjusted_rand'}\n",
      "extra_params: {}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9407c51a97654108bb93691403a94213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Combinations completed:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 14:17:30\n",
      "Running...\n",
      "model: BatchCoHiRF-SC-SRGF\n",
      "seed_model: 0\n",
      "dataset_name: criteo-up-sub\n",
      "timeout_fit: None\n",
      "timeout_combination: None\n",
      "n_jobs: 10\n",
      "model_params: {'cohirf_kwargs': {'n_samples_representative': 10000}, 'n_batches': 100}\n",
      "max_threads: None\n",
      "calculate_davies_bouldin: False\n",
      "calculate_full_silhouette: False\n",
      "calculate_metrics_even_if_too_many_clusters: False\n",
      "standardize: False\n",
      "hpo_framework: optuna\n",
      "n_trials: 20\n",
      "timeout_hpo: 0\n",
      "timeout_trial: 0\n",
      "max_concurrent_trials: 1\n",
      "hpo_seed: 0\n",
      "sampler: tpe\n",
      "pruner: none\n",
      "direction: maximize\n",
      "hpo_metric: adjusted_rand\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f89320249c7249b391fa5199f82e717c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Trials:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m      1\u001b[39m experiment = HPOCSVClusteringExperiment(\n\u001b[32m      2\u001b[39m     \u001b[38;5;66;03m# hpo\u001b[39;00m\n\u001b[32m      3\u001b[39m     n_trials=\u001b[32m20\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m     **experiment_params,\n\u001b[32m     17\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m result = \u001b[43mexperiment\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreturn_results\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m     19\u001b[39m ari = result[\u001b[33m\"\u001b[39m\u001b[33mevaluate_model_return\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mbest/adjusted_rand\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     20\u001b[39m hpo_time = result[\u001b[33m\"\u001b[39m\u001b[33mfit_model_return\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33melapsed_time\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/ml_experiments/ml_experiments/base_experiment.py:1571\u001b[39m, in \u001b[36mBaseExperiment.run\u001b[39m\u001b[34m(self, return_results)\u001b[39m\n\u001b[32m   1569\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1570\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1571\u001b[39m     results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1572\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/ml_experiments/ml_experiments/base_experiment.py:1503\u001b[39m, in \u001b[36mBaseExperiment._run_experiment\u001b[39m\u001b[34m(self, client, return_results)\u001b[39m\n\u001b[32m   1501\u001b[39m     combination = \u001b[38;5;28mlist\u001b[39m(combination) + [run_id]\n\u001b[32m   1502\u001b[39m     combination_names.append(\u001b[33m\"\u001b[39m\u001b[33mmlflow_run_id\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1503\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_combination\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1504\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43mcombination\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1505\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcombination_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcombination_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1506\u001b[39m \u001b[43m    \u001b[49m\u001b[43munique_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43munique_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1507\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextra_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1508\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1509\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_results:\n\u001b[32m   1511\u001b[39m     combination_success = result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/ml_experiments/ml_experiments/base_experiment.py:1300\u001b[39m, in \u001b[36mBaseExperiment._run_combination\u001b[39m\u001b[34m(self, combination_names, unique_params, extra_params, return_results, *combination, **kwargs)\u001b[39m\n\u001b[32m   1298\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1299\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1300\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs_fn\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/ml_experiments/ml_experiments/base_experiment.py:1247\u001b[39m, in \u001b[36mBaseExperiment._run_mlflow_and_train_model\u001b[39m\u001b[34m(self, combination, unique_params, extra_params, return_results, mlflow_run_id, **kwargs)\u001b[39m\n\u001b[32m   1237\u001b[39m mlflow_client.update_run(mlflow_run_id, status=\u001b[33m\"\u001b[39m\u001b[33mRUNNING\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1238\u001b[39m \u001b[38;5;28mself\u001b[39m._log_run_start_params(\n\u001b[32m   1239\u001b[39m     combination=combination,\n\u001b[32m   1240\u001b[39m     unique_params=unique_params,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1244\u001b[39m     **kwargs,\n\u001b[32m   1245\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1247\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1248\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcombination\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcombination\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1249\u001b[39m \u001b[43m    \u001b[49m\u001b[43munique_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43munique_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1250\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1251\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextra_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1252\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmlflow_run_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmlflow_run_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1253\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1254\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/ml_experiments/ml_experiments/base_experiment.py:1010\u001b[39m, in \u001b[36mBaseExperiment._train_model\u001b[39m\u001b[34m(self, combination, unique_params, extra_params, return_results, mlflow_run_id, **kwargs)\u001b[39m\n\u001b[32m   1008\u001b[39m         results[\u001b[33m\"\u001b[39m\u001b[33mfit_model_return\u001b[39m\u001b[33m\"\u001b[39m] = ret\n\u001b[32m   1009\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1010\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcombination\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcombination\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43munique_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43munique_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmlflow_run_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmlflow_run_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1018\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ret:\n\u001b[32m   1019\u001b[39m         results[\u001b[33m\"\u001b[39m\u001b[33mfit_model_return\u001b[39m\u001b[33m\"\u001b[39m] = ret\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/ml_experiments/ml_experiments/utils.py:131\u001b[39m, in \u001b[36mprofile_time.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    130\u001b[39m start_time = perf_counter()\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m elapsed_time = perf_counter() - start_time\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_in_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mdict\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/ml_experiments/ml_experiments/utils.py:174\u001b[39m, in \u001b[36mprofile_memory.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs_from_func)\u001b[39m\n\u001b[32m    172\u001b[39m     dynamic_enable = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dynamic_enable:\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs_from_func\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m retval:\n\u001b[32m    177\u001b[39m     mem_usage, result = memory_usage((func, args, kwargs_from_func), max_usage=max_usage, retval=retval, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/ml_experiments/ml_experiments/hpo_experiment.py:234\u001b[39m, in \u001b[36mHPOExperiment._fit_model\u001b[39m\u001b[34m(self, combination, unique_params, extra_params, mlflow_run_id, **kwargs)\u001b[39m\n\u001b[32m    222\u001b[39m default_values = \u001b[38;5;28mself\u001b[39m.get_default_values(combination, unique_params, extra_params, mlflow_run_id, **kwargs)\n\u001b[32m    223\u001b[39m get_trial_fn = partial(\n\u001b[32m    224\u001b[39m     \u001b[38;5;28mself\u001b[39m.get_trial_fn,\n\u001b[32m    225\u001b[39m     search_space=search_space,\n\u001b[32m   (...)\u001b[39m\u001b[32m    231\u001b[39m     **kwargs,\n\u001b[32m    232\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m study = \u001b[43mtuner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtune\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m    \u001b[49m\u001b[43msearch_space\u001b[49m\u001b[43m=\u001b[49m\u001b[43msearch_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdirection\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdirection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhpo_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m    \u001b[49m\u001b[43menqueue_configurations\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_trial_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_trial_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcombination\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcombination\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m    \u001b[49m\u001b[43munique_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43munique_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextra_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmlflow_run_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmlflow_run_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m    \u001b[49m\u001b[43mleave_pbar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    249\u001b[39m grid_search_stopped = tuner.grid_search_stopped\n\u001b[32m    250\u001b[39m elapsed_time_timed_out = tuner.timed_out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/ml_experiments/ml_experiments/tuners.py:212\u001b[39m, in \u001b[36mOptunaTuner.tune\u001b[39m\u001b[34m(self, training_fn, search_space, direction, metric, enqueue_configurations, get_trial_fn, hyperband_max_resources, leave_pbar, **kwargs)\u001b[39m\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    210\u001b[39m     max_trials_to_run = \u001b[38;5;28mself\u001b[39m.n_trials - n_trial\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m trials_numbers, results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_trials\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m    \u001b[49m\u001b[43msearch_space\u001b[49m\u001b[43m=\u001b[49m\u001b[43msearch_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_trial_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_trial_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_trials_to_run\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_trials_to_run\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m n_trials_run = \u001b[38;5;28mlen\u001b[39m(trials_numbers)\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m enqueued_configs > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/ml_experiments/ml_experiments/tuners.py:182\u001b[39m, in \u001b[36mOptunaTuner.run_trials\u001b[39m\u001b[34m(self, training_fn, search_space, get_trial_fn, max_trials_to_run, **kwargs)\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_trials\u001b[39m(\u001b[38;5;28mself\u001b[39m, training_fn, search_space, get_trial_fn, max_trials_to_run, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_simple_sequential_trial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtraining_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_space\u001b[49m\u001b[43m=\u001b[49m\u001b[43msearch_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_trial_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_trial_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/ml_experiments/ml_experiments/tuners.py:172\u001b[39m, in \u001b[36mOptunaTuner.run_simple_sequential_trial\u001b[39m\u001b[34m(self, training_fn, search_space, get_trial_fn, **kwargs)\u001b[39m\n\u001b[32m    170\u001b[39m         result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m     result = \u001b[43mtraining_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(trial, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    174\u001b[39m     trial_number = trial[\u001b[33m\"\u001b[39m\u001b[33mtrial\u001b[39m\u001b[33m\"\u001b[39m].number\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/cohirf/cohirf/experiment/hpo_clustering_experiment.py:116\u001b[39m, in \u001b[36mHPOClusteringExperiment.training_fn\u001b[39m\u001b[34m(self, trial_dict, combination, unique_params, extra_params, mlflow_run_id, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m         combination[\u001b[33m\"\u001b[39m\u001b[33mseed_model\u001b[39m\u001b[33m\"\u001b[39m] = seed_model\n\u001b[32m    115\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m mlflow_run_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m             results = \u001b[43msimple_experiment\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_run_mlflow_and_train_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m\t\t\t\t\u001b[49m\u001b[43mcombination\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcombination\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m\t\t\t\t\u001b[49m\u001b[43munique_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43munique_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[43m\t\t\t\t\u001b[49m\u001b[43mextra_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m\t\t\t\t\u001b[49m\u001b[43mmlflow_run_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchild_run_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m\t\t\t\t\u001b[49m\u001b[43mreturn_results\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m\t\t\t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    124\u001b[39m             results = simple_experiment._train_model(\n\u001b[32m    125\u001b[39m \t\t\t\tcombination=combination,\n\u001b[32m    126\u001b[39m \t\t\t\tunique_params=unique_params,\n\u001b[32m   (...)\u001b[39m\u001b[32m    129\u001b[39m \t\t\t\treturn_results=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    130\u001b[39m \t\t\t)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/ml_experiments/ml_experiments/base_experiment.py:1247\u001b[39m, in \u001b[36mBaseExperiment._run_mlflow_and_train_model\u001b[39m\u001b[34m(self, combination, unique_params, extra_params, return_results, mlflow_run_id, **kwargs)\u001b[39m\n\u001b[32m   1237\u001b[39m mlflow_client.update_run(mlflow_run_id, status=\u001b[33m\"\u001b[39m\u001b[33mRUNNING\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1238\u001b[39m \u001b[38;5;28mself\u001b[39m._log_run_start_params(\n\u001b[32m   1239\u001b[39m     combination=combination,\n\u001b[32m   1240\u001b[39m     unique_params=unique_params,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1244\u001b[39m     **kwargs,\n\u001b[32m   1245\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1247\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1248\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcombination\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcombination\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1249\u001b[39m \u001b[43m    \u001b[49m\u001b[43munique_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43munique_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1250\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1251\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextra_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1252\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmlflow_run_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmlflow_run_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1253\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1254\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/ml_experiments/ml_experiments/base_experiment.py:1010\u001b[39m, in \u001b[36mBaseExperiment._train_model\u001b[39m\u001b[34m(self, combination, unique_params, extra_params, return_results, mlflow_run_id, **kwargs)\u001b[39m\n\u001b[32m   1008\u001b[39m         results[\u001b[33m\"\u001b[39m\u001b[33mfit_model_return\u001b[39m\u001b[33m\"\u001b[39m] = ret\n\u001b[32m   1009\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1010\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcombination\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcombination\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43munique_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43munique_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmlflow_run_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmlflow_run_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1018\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ret:\n\u001b[32m   1019\u001b[39m         results[\u001b[33m\"\u001b[39m\u001b[33mfit_model_return\u001b[39m\u001b[33m\"\u001b[39m] = ret\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/ml_experiments/ml_experiments/utils.py:131\u001b[39m, in \u001b[36mprofile_time.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    130\u001b[39m start_time = perf_counter()\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m elapsed_time = perf_counter() - start_time\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_in_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mdict\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/ml_experiments/ml_experiments/utils.py:174\u001b[39m, in \u001b[36mprofile_memory.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs_from_func)\u001b[39m\n\u001b[32m    172\u001b[39m     dynamic_enable = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dynamic_enable:\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs_from_func\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m retval:\n\u001b[32m    177\u001b[39m     mem_usage, result = memory_usage((func, args, kwargs_from_func), max_usage=max_usage, retval=retval, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/cohirf/cohirf/experiment/clustering_experiment.py:239\u001b[39m, in \u001b[36mClusteringExperiment._fit_model\u001b[39m\u001b[34m(self, combination, unique_params, extra_params, mlflow_run_id, **kwargs)\u001b[39m\n\u001b[32m    237\u001b[39m model = kwargs[\u001b[33m'\u001b[39m\u001b[33mload_model_return\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    238\u001b[39m X = kwargs[\u001b[33m'\u001b[39m\u001b[33mload_data_return\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mX\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m y_pred = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m'\u001b[39m\u001b[33my_pred\u001b[39m\u001b[33m'\u001b[39m: y_pred}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/cohirf/cohirf/models/batch_cohirf.py:292\u001b[39m, in \u001b[36mBatchCoHiRF.fit_predict\u001b[39m\u001b[34m(self, X, y, sample_weight, representatives_indexes, parents, labels)\u001b[39m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit_predict\u001b[39m(\n\u001b[32m    284\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    285\u001b[39m     X: pd.DataFrame | np.ndarray,\n\u001b[32m   (...)\u001b[39m\u001b[32m    290\u001b[39m     labels=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    291\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepresentatives_indexes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.automatically_get_labels:\n\u001b[32m    294\u001b[39m         \u001b[38;5;28mself\u001b[39m.get_labels()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/cohirf/cohirf/models/batch_cohirf.py:244\u001b[39m, in \u001b[36mBatchCoHiRF.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, representatives_indexes, parents, labels)\u001b[39m\n\u001b[32m    234\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStarting epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    236\u001b[39m X_representatives = \u001b[38;5;28mself\u001b[39m.get_X_representatives(X_representatives, representatives_local_indexes)\n\u001b[32m    238\u001b[39m (\n\u001b[32m    239\u001b[39m     new_representatives_local_indexes,\n\u001b[32m    240\u001b[39m     new_local_parents,\n\u001b[32m    241\u001b[39m     new_local_labels,\n\u001b[32m    242\u001b[39m     new_n_clusters,\n\u001b[32m    243\u001b[39m     stop,\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_representatives\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.hierarchy_strategy == \u001b[33m\"\u001b[39m\u001b[33mparents\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    247\u001b[39m     new_absolute_parents = representatives_absolute_indexes[new_local_parents]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/cohirf/cohirf/models/batch_cohirf.py:87\u001b[39m, in \u001b[36mBatchCoHiRF.run_one_epoch\u001b[39m\u001b[34m(self, X_representatives)\u001b[39m\n\u001b[32m     84\u001b[39m     last_epoch = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     86\u001b[39m parallel = Parallel(n_jobs=\u001b[38;5;28mself\u001b[39m.n_jobs, return_as=\u001b[33m\"\u001b[39m\u001b[33mlist\u001b[39m\u001b[33m\"\u001b[39m, verbose=\u001b[38;5;28mself\u001b[39m.verbose)\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m results = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_one_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_representatives\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_batches\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m all_parents, all_labels, all_representatives_indexes, all_n_clusters = \u001b[38;5;28mzip\u001b[39m(*results)\n\u001b[32m     89\u001b[39m all_parents = \u001b[38;5;28mlist\u001b[39m(all_parents)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cohirf/lib/python3.11/site-packages/joblib/parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cohirf/lib/python3.11/site-packages/joblib/parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cohirf/lib/python3.11/site-packages/joblib/parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         time.sleep(\u001b[32m0.01\u001b[39m)\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "experiment = HPOCSVClusteringExperiment(\n",
    "    # hpo\n",
    "    n_trials=20,\n",
    "    hpo_seed=0,\n",
    "    hpo_metric=\"adjusted_rand\",\n",
    "    direction=\"maximize\",\n",
    "    # model\n",
    "    experiment_name=\"test-criteo-sub\",\n",
    "    model=\"BatchCoHiRF-SC-SRGF\",\n",
    "    seed_model=0,\n",
    "    model_params=dict(cohirf_kwargs=dict(n_samples_representative=10000), n_batches=100),\n",
    "    n_jobs=10,\n",
    "    # dataset\n",
    "    dataset_name=\"criteo-up-sub\",\n",
    "    raise_on_error=True,\n",
    "    **experiment_params,\n",
    ")\n",
    "result = experiment.run(return_results=True)[0]\n",
    "ari = result[\"evaluate_model_return\"][\"best/adjusted_rand\"]\n",
    "hpo_time = result[\"fit_model_return\"][\"elapsed_time\"]\n",
    "best_time = result[\"evaluate_model_return\"][\"best/elapsed_time\"]\n",
    "print(f\"ARI: {ari:.4f}, HPO time: {hpo_time:.2f}s, Best time: {best_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab2e0de",
   "metadata": {},
   "source": [
    "# KDD-99 simplified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13f09937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 17:42:49\n",
      "Starting experiment...\n",
      "combination_names: ['model', 'seed_model', 'dataset_name']\n",
      "combinations: [('KMeans', 0, 'kdd-99-sub')]\n",
      "unique_params: {'timeout_fit': None, 'timeout_combination': None, 'n_jobs': 1, 'model_params': {}, 'max_threads': None, 'calculate_davies_bouldin': False, 'calculate_full_silhouette': False, 'calculate_metrics_even_if_too_many_clusters': False, 'standardize': False, 'hpo_framework': 'optuna', 'n_trials': 20, 'timeout_hpo': 0, 'timeout_trial': 0, 'max_concurrent_trials': 1, 'hpo_seed': 0, 'sampler': 'tpe', 'pruner': 'none', 'direction': 'maximize', 'hpo_metric': 'adjusted_rand'}\n",
      "extra_params: {}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea40ea219afe420b91352ed796ce4983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Combinations completed:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/11 17:42:50 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2025/08/11 17:42:50 INFO mlflow.store.db.utils: Updating database tables\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 17:42:51\n",
      "Running...\n",
      "model: KMeans\n",
      "seed_model: 0\n",
      "dataset_name: kdd-99-sub\n",
      "timeout_fit: None\n",
      "timeout_combination: None\n",
      "n_jobs: 1\n",
      "model_params: {}\n",
      "max_threads: None\n",
      "calculate_davies_bouldin: False\n",
      "calculate_full_silhouette: False\n",
      "calculate_metrics_even_if_too_many_clusters: False\n",
      "standardize: False\n",
      "hpo_framework: optuna\n",
      "n_trials: 20\n",
      "timeout_hpo: 0\n",
      "timeout_trial: 0\n",
      "max_concurrent_trials: 1\n",
      "hpo_seed: 0\n",
      "sampler: tpe\n",
      "pruner: none\n",
      "direction: maximize\n",
      "hpo_metric: adjusted_rand\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4c1fbef32034e06ba1be987a3da1b34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Trials:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 17:43:12\n",
      "Finished!\n",
      "total_elapsed_time: 20.98381746999803\n",
      "model: KMeans\n",
      "seed_model: 0\n",
      "dataset_name: kdd-99-sub\n",
      "timeout_fit: None\n",
      "timeout_combination: None\n",
      "n_jobs: 1\n",
      "model_params: {'n_clusters': 23}\n",
      "max_threads: None\n",
      "calculate_davies_bouldin: False\n",
      "calculate_full_silhouette: False\n",
      "calculate_metrics_even_if_too_many_clusters: False\n",
      "standardize: False\n",
      "hpo_framework: optuna\n",
      "n_trials: 20\n",
      "timeout_hpo: 0\n",
      "timeout_trial: 0\n",
      "max_concurrent_trials: 1\n",
      "hpo_seed: 0\n",
      "sampler: tpe\n",
      "pruner: none\n",
      "direction: maximize\n",
      "hpo_metric: adjusted_rand\n",
      "\n",
      "2025-08-11 17:43:12\n",
      "Combinations completed:   0%|          | 0/1 [00:22<?, ?it/s]\n",
      "succesfully_completed: 1\n",
      "failed: 0\n",
      "none: 0\n",
      "\n",
      "ARI: 0.6991, HPO time: 20.49s, Best time: 0.44s\n"
     ]
    }
   ],
   "source": [
    "experiment = HPOCSVClusteringExperiment(\n",
    "    # hpo\n",
    "    n_trials=20,\n",
    "    hpo_seed=0,\n",
    "    hpo_metric=\"adjusted_rand\",\n",
    "    direction=\"maximize\",\n",
    "    # model\n",
    "    experiment_name=\"test-kdd-sub\",\n",
    "    model=\"KMeans\",\n",
    "    seed_model=0,\n",
    "    # dataset\n",
    "    dataset_name=\"kdd-99-sub\", \n",
    "\traise_on_error=True,\n",
    "    **experiment_params,\n",
    ")\n",
    "result = experiment.run(return_results=True)[0]\n",
    "ari = result[\"evaluate_model_return\"][\"best/adjusted_rand\"]\n",
    "hpo_time = result[\"fit_model_return\"][\"elapsed_time\"]\n",
    "best_time = result[\"evaluate_model_return\"][\"best/elapsed_time\"]\n",
    "print(f\"ARI: {ari:.4f}, HPO time: {hpo_time:.2f}s, Best time: {best_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "263ab6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARI: 0.6991, HPO time: 20.25s, Best time: 0.24s\n"
     ]
    }
   ],
   "source": [
    "print(f\"ARI: {ari:.4f}, HPO time: {hpo_time:.2f}s, Best time: {best_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c2adb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 15:06:52\n",
      "Starting experiment...\n",
      "combination_names: ['model', 'seed_model', 'dataset_name']\n",
      "combinations: [('BatchCoHiRF-1iter', 0, 'kdd-99-full')]\n",
      "unique_params: {'timeout_fit': None, 'timeout_combination': None, 'n_jobs': 1, 'model_params': {}, 'max_threads': None, 'calculate_davies_bouldin': False, 'calculate_full_silhouette': False, 'calculate_metrics_even_if_too_many_clusters': False, 'standardize': False, 'hpo_framework': 'optuna', 'n_trials': 20, 'timeout_hpo': 0, 'timeout_trial': 0, 'max_concurrent_trials': 1, 'hpo_seed': 0, 'sampler': 'tpe', 'pruner': 'none', 'direction': 'maximize', 'hpo_metric': 'adjusted_rand'}\n",
      "extra_params: {}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab0941c7462d45f3b4f7c27b4af0dc44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Combinations completed:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 15:06:53\n",
      "Running...\n",
      "model: BatchCoHiRF-1iter\n",
      "seed_model: 0\n",
      "dataset_name: kdd-99-full\n",
      "timeout_fit: None\n",
      "timeout_combination: None\n",
      "n_jobs: 1\n",
      "model_params: {}\n",
      "max_threads: None\n",
      "calculate_davies_bouldin: False\n",
      "calculate_full_silhouette: False\n",
      "calculate_metrics_even_if_too_many_clusters: False\n",
      "standardize: False\n",
      "hpo_framework: optuna\n",
      "n_trials: 20\n",
      "timeout_hpo: 0\n",
      "timeout_trial: 0\n",
      "max_concurrent_trials: 1\n",
      "hpo_seed: 0\n",
      "sampler: tpe\n",
      "pruner: none\n",
      "direction: maximize\n",
      "hpo_metric: adjusted_rand\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaaaf80f47134268b724297b7370003a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Trials:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment = HPOCSVClusteringExperiment(\n",
    "    # hpo\n",
    "    n_trials=20,\n",
    "    hpo_seed=0,\n",
    "    hpo_metric=\"adjusted_rand\",\n",
    "    direction=\"maximize\",\n",
    "    # model\n",
    "    experiment_name=\"test-kdd-sub\",\n",
    "    model=\"BatchCoHiRF-1iter\",\n",
    "    seed_model=0,\n",
    "    # dataset\n",
    "    dataset_name=\"kdd-99-full\",\n",
    "    raise_on_error=True,\n",
    "    **experiment_params,\n",
    ")\n",
    "result = experiment.run(return_results=True)[0]\n",
    "ari = result[\"evaluate_model_return\"][\"best/adjusted_rand\"]\n",
    "hpo_time = result[\"fit_model_return\"][\"elapsed_time\"]\n",
    "best_time = result[\"evaluate_model_return\"][\"best/elapsed_time\"]\n",
    "print(f\"ARI: {ari:.4f}, HPO time: {hpo_time:.2f}s, Best time: {best_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894fde54",
   "metadata": {},
   "source": [
    "# User behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bda728f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-12 21:11:59\n",
      "Starting experiment...\n",
      "combination_names: ['model', 'seed_model', 'dataset_name']\n",
      "combinations: [('KMeans', 0, 'user-behavior-100k')]\n",
      "unique_params: {'timeout_fit': None, 'timeout_combination': None, 'n_jobs': 1, 'model_params': {}, 'max_threads': None, 'calculate_davies_bouldin': False, 'calculate_full_silhouette': False, 'calculate_metrics_even_if_too_many_clusters': False, 'standardize': True, 'hpo_framework': 'optuna', 'n_trials': 20, 'timeout_hpo': 0, 'timeout_trial': 0, 'max_concurrent_trials': 1, 'hpo_seed': 0, 'sampler': 'tpe', 'pruner': 'none', 'direction': 'maximize', 'hpo_metric': 'adjusted_rand'}\n",
      "extra_params: {}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20f579408bef431bb7de02517fcc8329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Combinations completed:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-12 21:12:00\n",
      "Running...\n",
      "model: KMeans\n",
      "seed_model: 0\n",
      "dataset_name: user-behavior-100k\n",
      "timeout_fit: None\n",
      "timeout_combination: None\n",
      "n_jobs: 1\n",
      "model_params: {}\n",
      "max_threads: None\n",
      "calculate_davies_bouldin: False\n",
      "calculate_full_silhouette: False\n",
      "calculate_metrics_even_if_too_many_clusters: False\n",
      "standardize: True\n",
      "hpo_framework: optuna\n",
      "n_trials: 20\n",
      "timeout_hpo: 0\n",
      "timeout_trial: 0\n",
      "max_concurrent_trials: 1\n",
      "hpo_seed: 0\n",
      "sampler: tpe\n",
      "pruner: none\n",
      "direction: maximize\n",
      "hpo_metric: adjusted_rand\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54ca672855de4723bdc95a11cf637545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Trials:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-12 21:12:49\n",
      "Finished!\n",
      "total_elapsed_time: 49.53405673499947\n",
      "model: KMeans\n",
      "seed_model: 0\n",
      "dataset_name: user-behavior-100k\n",
      "timeout_fit: None\n",
      "timeout_combination: None\n",
      "n_jobs: 1\n",
      "model_params: {'n_clusters': 12}\n",
      "max_threads: None\n",
      "calculate_davies_bouldin: False\n",
      "calculate_full_silhouette: False\n",
      "calculate_metrics_even_if_too_many_clusters: False\n",
      "standardize: True\n",
      "hpo_framework: optuna\n",
      "n_trials: 20\n",
      "timeout_hpo: 0\n",
      "timeout_trial: 0\n",
      "max_concurrent_trials: 1\n",
      "hpo_seed: 0\n",
      "sampler: tpe\n",
      "pruner: none\n",
      "direction: maximize\n",
      "hpo_metric: adjusted_rand\n",
      "\n",
      "2025-08-12 21:12:49\n",
      "Combinations completed:   0%|          | 0/1 [00:49<?, ?it/s]\n",
      "succesfully_completed: 1\n",
      "failed: 0\n",
      "none: 0\n",
      "\n",
      "ARI: 0.3964, HPO time: 48.43s, Best time: 0.22s\n"
     ]
    }
   ],
   "source": [
    "experiment = HPOCSVClusteringExperiment(\n",
    "    # hpo\n",
    "    n_trials=20,\n",
    "    hpo_seed=0,\n",
    "    hpo_metric=\"adjusted_rand\",\n",
    "    direction=\"maximize\",\n",
    "    # model\n",
    "    experiment_name=\"test-user-behavior-100k\",\n",
    "    model=\"KMeans\",\n",
    "    seed_model=0,\n",
    "    # dataset\n",
    "    dataset_name=\"user-behavior-100k\",\n",
    "\tstandardize=True,\n",
    "\traise_on_error=True,\n",
    "    **experiment_params,\n",
    ")\n",
    "result = experiment.run(return_results=True)[0]\n",
    "ari = result[\"evaluate_model_return\"][\"best/adjusted_rand\"]\n",
    "hpo_time = result[\"fit_model_return\"][\"elapsed_time\"]\n",
    "best_time = result[\"evaluate_model_return\"][\"best/elapsed_time\"]\n",
    "print(f\"ARI: {ari:.4f}, HPO time: {hpo_time:.2f}s, Best time: {best_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed3f0749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-12 21:04:19\n",
      "Starting experiment...\n",
      "combination_names: ['model', 'seed_model', 'dataset_name']\n",
      "combinations: [('BatchCoHiRF-1iter', 0, 'user-behavior-100k')]\n",
      "unique_params: {'timeout_fit': None, 'timeout_combination': None, 'n_jobs': 10, 'model_params': {}, 'max_threads': None, 'calculate_davies_bouldin': False, 'calculate_full_silhouette': False, 'calculate_metrics_even_if_too_many_clusters': False, 'standardize': True, 'hpo_framework': 'optuna', 'n_trials': 50, 'timeout_hpo': 0, 'timeout_trial': 0, 'max_concurrent_trials': 1, 'hpo_seed': 0, 'sampler': 'tpe', 'pruner': 'none', 'direction': 'maximize', 'hpo_metric': 'adjusted_rand'}\n",
      "extra_params: {}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d4b1198967b4cf1ad520dd253258220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Combinations completed:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-12 21:04:21\n",
      "Running...\n",
      "model: BatchCoHiRF-1iter\n",
      "seed_model: 0\n",
      "dataset_name: user-behavior-100k\n",
      "timeout_fit: None\n",
      "timeout_combination: None\n",
      "n_jobs: 10\n",
      "model_params: {}\n",
      "max_threads: None\n",
      "calculate_davies_bouldin: False\n",
      "calculate_full_silhouette: False\n",
      "calculate_metrics_even_if_too_many_clusters: False\n",
      "standardize: True\n",
      "hpo_framework: optuna\n",
      "n_trials: 50\n",
      "timeout_hpo: 0\n",
      "timeout_trial: 0\n",
      "max_concurrent_trials: 1\n",
      "hpo_seed: 0\n",
      "sampler: tpe\n",
      "pruner: none\n",
      "direction: maximize\n",
      "hpo_metric: adjusted_rand\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcaf6ca0ef584dcb98c8867e9a22b0d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Trials:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-12 21:10:02\n",
      "Finished!\n",
      "total_elapsed_time: 347.3877117869997\n",
      "model: BatchCoHiRF-1iter\n",
      "seed_model: 0\n",
      "dataset_name: user-behavior-100k\n",
      "timeout_fit: None\n",
      "timeout_combination: None\n",
      "n_jobs: 10\n",
      "model_params: {'cohirf_kwargs': {'n_features': 0.5868618227602919, 'repetitions': 2, 'kmeans_n_clusters': 2}}\n",
      "max_threads: None\n",
      "calculate_davies_bouldin: False\n",
      "calculate_full_silhouette: False\n",
      "calculate_metrics_even_if_too_many_clusters: False\n",
      "standardize: True\n",
      "hpo_framework: optuna\n",
      "n_trials: 50\n",
      "timeout_hpo: 0\n",
      "timeout_trial: 0\n",
      "max_concurrent_trials: 1\n",
      "hpo_seed: 0\n",
      "sampler: tpe\n",
      "pruner: none\n",
      "direction: maximize\n",
      "hpo_metric: adjusted_rand\n",
      "\n",
      "2025-08-12 21:10:02\n",
      "Combinations completed:   0%|          | 0/1 [05:42<?, ?it/s]\n",
      "succesfully_completed: 1\n",
      "failed: 0\n",
      "none: 0\n",
      "\n",
      "ARI: 0.2618, HPO time: 346.33s, Best time: 0.64s\n"
     ]
    }
   ],
   "source": [
    "experiment = HPOCSVClusteringExperiment(\n",
    "    # hpo\n",
    "    n_trials=50,\n",
    "    hpo_seed=0,\n",
    "    hpo_metric=\"adjusted_rand\",\n",
    "    direction=\"maximize\",\n",
    "    # model\n",
    "    experiment_name=\"test-user-behavior-100k\",\n",
    "    model=\"BatchCoHiRF-1iter\",\n",
    "    seed_model=0,\n",
    "\tn_jobs=10,\n",
    "    # dataset\n",
    "    dataset_name=\"user-behavior-100k\",\n",
    "\tstandardize=True,\n",
    "    raise_on_error=True,\n",
    "    **experiment_params,\n",
    ")\n",
    "result = experiment.run(return_results=True)[0]\n",
    "ari = result[\"evaluate_model_return\"][\"best/adjusted_rand\"]\n",
    "hpo_time = result[\"fit_model_return\"][\"elapsed_time\"]\n",
    "best_time = result[\"evaluate_model_return\"][\"best/elapsed_time\"]\n",
    "print(f\"ARI: {ari:.4f}, HPO time: {hpo_time:.2f}s, Best time: {best_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55677bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-12 21:23:20\n",
      "Starting experiment...\n",
      "combination_names: ['model', 'seed_model', 'dataset_name']\n",
      "combinations: [('BatchCoHiRF-SC-SRGF', 0, 'user-behavior-100k')]\n",
      "unique_params: {'timeout_fit': None, 'timeout_combination': None, 'n_jobs': 10, 'model_params': {'cohirf_kwargs': {'n_samples_representative': 10000}, 'n_batches': 100}, 'max_threads': None, 'calculate_davies_bouldin': False, 'calculate_full_silhouette': False, 'calculate_metrics_even_if_too_many_clusters': False, 'standardize': False, 'hpo_framework': 'optuna', 'n_trials': 30, 'timeout_hpo': 0, 'timeout_trial': 0, 'max_concurrent_trials': 1, 'hpo_seed': 0, 'sampler': 'tpe', 'pruner': 'none', 'direction': 'maximize', 'hpo_metric': 'adjusted_rand'}\n",
      "extra_params: {}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be02fd8b38f5419b831c5e717df3ad67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Combinations completed:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-12 21:23:21\n",
      "Running...\n",
      "model: BatchCoHiRF-SC-SRGF\n",
      "seed_model: 0\n",
      "dataset_name: user-behavior-100k\n",
      "timeout_fit: None\n",
      "timeout_combination: None\n",
      "n_jobs: 10\n",
      "model_params: {'cohirf_kwargs': {'n_samples_representative': 10000}, 'n_batches': 100}\n",
      "max_threads: None\n",
      "calculate_davies_bouldin: False\n",
      "calculate_full_silhouette: False\n",
      "calculate_metrics_even_if_too_many_clusters: False\n",
      "standardize: False\n",
      "hpo_framework: optuna\n",
      "n_trials: 30\n",
      "timeout_hpo: 0\n",
      "timeout_trial: 0\n",
      "max_concurrent_trials: 1\n",
      "hpo_seed: 0\n",
      "sampler: tpe\n",
      "pruner: none\n",
      "direction: maximize\n",
      "hpo_metric: adjusted_rand\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1d8ee5625ff4075a66eef7fdd5cb474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Trials:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment = HPOCSVClusteringExperiment(\n",
    "    # hpo\n",
    "    n_trials=30,\n",
    "    hpo_seed=0,\n",
    "    hpo_metric=\"adjusted_rand\",\n",
    "    direction=\"maximize\",\n",
    "    # model\n",
    "    experiment_name=\"test-user-behavior-100k\",\n",
    "    model=\"BatchCoHiRF-SC-SRGF\",\n",
    "    model_params=dict(cohirf_kwargs=dict(n_samples_representative=10000), n_batches=100),\n",
    "    seed_model=0,\n",
    "    n_jobs=10,\n",
    "    # dataset\n",
    "    dataset_name=\"user-behavior-100k\",\n",
    "    # standardize=True,\n",
    "    raise_on_error=True,\n",
    "    **experiment_params,\n",
    ")\n",
    "result = experiment.run(return_results=True)[0]\n",
    "ari = result[\"evaluate_model_return\"][\"best/adjusted_rand\"]\n",
    "hpo_time = result[\"fit_model_return\"][\"elapsed_time\"]\n",
    "best_time = result[\"evaluate_model_return\"][\"best/elapsed_time\"]\n",
    "print(f\"ARI: {ari:.4f}, HPO time: {hpo_time:.2f}s, Best time: {best_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60803102",
   "metadata": {},
   "source": [
    "# Wine variety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9734ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-19 11:26:58\n",
      "Starting experiment...\n",
      "combination_names: ['model', 'seed_model', 'dataset_name']\n",
      "combinations: [('KMeans', 0, 'wine')]\n",
      "unique_params: {'timeout_fit': None, 'timeout_combination': None, 'n_jobs': 1, 'model_params': {}, 'max_threads': None, 'calculate_davies_bouldin': False, 'calculate_full_silhouette': False, 'calculate_metrics_even_if_too_many_clusters': False, 'standardize': False, 'hpo_framework': 'optuna', 'n_trials': 20, 'timeout_hpo': 0, 'timeout_trial': 0, 'max_concurrent_trials': 1, 'hpo_seed': 0, 'sampler': 'tpe', 'pruner': 'none', 'direction': 'maximize', 'hpo_metric': 'adjusted_rand'}\n",
      "extra_params: {}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac44d89ddaa445f0952713020c699bc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Combinations completed:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/19 11:26:59 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2025/08/19 11:26:59 INFO mlflow.store.db.utils: Updating database tables\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-19 11:27:00\n",
      "Running...\n",
      "model: KMeans\n",
      "seed_model: 0\n",
      "dataset_name: wine\n",
      "timeout_fit: None\n",
      "timeout_combination: None\n",
      "n_jobs: 1\n",
      "model_params: {}\n",
      "max_threads: None\n",
      "calculate_davies_bouldin: False\n",
      "calculate_full_silhouette: False\n",
      "calculate_metrics_even_if_too_many_clusters: False\n",
      "standardize: False\n",
      "hpo_framework: optuna\n",
      "n_trials: 20\n",
      "timeout_hpo: 0\n",
      "timeout_trial: 0\n",
      "max_concurrent_trials: 1\n",
      "hpo_seed: 0\n",
      "sampler: tpe\n",
      "pruner: none\n",
      "direction: maximize\n",
      "hpo_metric: adjusted_rand\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7944ddcf4b743d8b02e2a16987d54f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Trials:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-19 11:41:34\n",
      "Finished!\n",
      "total_elapsed_time: 883.1021250919998\n",
      "model: KMeans\n",
      "seed_model: 0\n",
      "dataset_name: wine\n",
      "timeout_fit: None\n",
      "timeout_combination: None\n",
      "n_jobs: 1\n",
      "model_params: {'n_clusters': 23}\n",
      "max_threads: None\n",
      "calculate_davies_bouldin: False\n",
      "calculate_full_silhouette: False\n",
      "calculate_metrics_even_if_too_many_clusters: False\n",
      "standardize: False\n",
      "hpo_framework: optuna\n",
      "n_trials: 20\n",
      "timeout_hpo: 0\n",
      "timeout_trial: 0\n",
      "max_concurrent_trials: 1\n",
      "hpo_seed: 0\n",
      "sampler: tpe\n",
      "pruner: none\n",
      "direction: maximize\n",
      "hpo_metric: adjusted_rand\n",
      "\n",
      "2025-08-19 11:41:34\n",
      "Combinations completed:   0%|          | 0/1 [14:36<?, ?it/s]\n",
      "succesfully_completed: 1\n",
      "failed: 0\n",
      "none: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiment = HPOCSVClusteringExperiment(\n",
    "    # hpo\n",
    "    n_trials=20,\n",
    "    hpo_seed=0,\n",
    "    hpo_metric=\"adjusted_rand\",\n",
    "    direction=\"maximize\",\n",
    "    # model\n",
    "    experiment_name=\"test-wine\",\n",
    "    model=\"KMeans\",\n",
    "    seed_model=0,\n",
    "    # dataset\n",
    "    dataset_name=\"wine\",\n",
    "    raise_on_error=True,\n",
    "    **experiment_params,\n",
    ")\n",
    "result = experiment.run(return_results=True)[0]\n",
    "ari = result[\"evaluate_model_return\"][\"best/adjusted_rand\"]\n",
    "hpo_time = result[\"fit_model_return\"][\"elapsed_time\"]\n",
    "best_time = result[\"evaluate_model_return\"][\"best/elapsed_time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df96365b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARI: 0.3097, HPO time: 851.13s, Best time: 8.08s\n"
     ]
    }
   ],
   "source": [
    "print(f\"ARI: {ari:.4f}, HPO time: {hpo_time:.2f}s, Best time: {best_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "015a4db7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c08bb25976ed4a3884cb31befe44a8b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Combinations completed:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/19 11:45:30 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2025/08/19 11:45:30 INFO mlflow.store.db.utils: Updating database tables\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daf5e7e6a9334407aa60721415bd0187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Trials:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARI: 0.2276, HPO time: 1203.40s, Best time: 24.01s\n"
     ]
    }
   ],
   "source": [
    "experiment = HPOCSVClusteringExperiment(\n",
    "    # hpo\n",
    "    n_trials=20,\n",
    "    hpo_seed=0,\n",
    "    hpo_metric=\"adjusted_rand\",\n",
    "    direction=\"maximize\",\n",
    "    # model\n",
    "    experiment_name=\"test-wine\",\n",
    "    model=\"CoHiRF\",\n",
    "\tmodel_params=dict(n_samples_representative=10000),\n",
    "    seed_model=0,\n",
    "    # dataset\n",
    "    dataset_name=\"wine\",\n",
    "    raise_on_error=True,\n",
    "\tverbose=1,\n",
    "    **experiment_params,\n",
    ")\n",
    "result = experiment.run(return_results=True)[0]\n",
    "ari = result[\"evaluate_model_return\"][\"best/adjusted_rand\"]\n",
    "hpo_time = result[\"fit_model_return\"][\"elapsed_time\"]\n",
    "best_time = result[\"evaluate_model_return\"][\"best/elapsed_time\"]\n",
    "print(f\"ARI: {ari:.4f}, HPO time: {hpo_time:.2f}s, Best time: {best_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2e6d5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cohirf.models.modal_cohirf import ModalCoHiRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d647a8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModalCoHiRF\n",
    "model_params = dict(\n",
    "    features_groups=[\n",
    "        [\n",
    "            0,\n",
    "            1,\n",
    "            2,\n",
    "            3,\n",
    "            4,\n",
    "            5,\n",
    "            6,\n",
    "            7,\n",
    "            8,\n",
    "            9,\n",
    "            10,\n",
    "            11,\n",
    "            12,\n",
    "            13,\n",
    "            14,\n",
    "            15,\n",
    "            16,\n",
    "            17,\n",
    "            18,\n",
    "            19,\n",
    "            20,\n",
    "            21,\n",
    "            22,\n",
    "            23,\n",
    "            24,\n",
    "            25,\n",
    "            26,\n",
    "            27,\n",
    "            28,\n",
    "            29,\n",
    "            30,\n",
    "            31,\n",
    "            32,\n",
    "            33,\n",
    "            34,\n",
    "            35,\n",
    "            36,\n",
    "            37,\n",
    "            38,\n",
    "            39,\n",
    "            40,\n",
    "            41,\n",
    "            42,\n",
    "            43,\n",
    "            44,\n",
    "            45,\n",
    "            46,\n",
    "            47,\n",
    "            48,\n",
    "            49,\n",
    "            50,\n",
    "            51,\n",
    "            52,\n",
    "            53,\n",
    "            54,\n",
    "            55,\n",
    "            56,\n",
    "            57,\n",
    "            58,\n",
    "            59,\n",
    "            60,\n",
    "            61,\n",
    "            62,\n",
    "            63,\n",
    "            64,\n",
    "            65,\n",
    "            66,\n",
    "            67,\n",
    "            68,\n",
    "            69,\n",
    "            70,\n",
    "            71,\n",
    "            72,\n",
    "            73,\n",
    "            74,\n",
    "            75,\n",
    "            76,\n",
    "            77,\n",
    "            78,\n",
    "            79,\n",
    "            80,\n",
    "            81,\n",
    "            82,\n",
    "            83,\n",
    "            84,\n",
    "            85,\n",
    "            86,\n",
    "            87,\n",
    "            88,\n",
    "            89,\n",
    "            90,\n",
    "            91,\n",
    "            92,\n",
    "            93,\n",
    "            94,\n",
    "            95,\n",
    "            96,\n",
    "            97,\n",
    "            98,\n",
    "            99,\n",
    "            100,\n",
    "            101,\n",
    "            102,\n",
    "            103,\n",
    "            104,\n",
    "            105,\n",
    "            106,\n",
    "            107,\n",
    "            108,\n",
    "            109,\n",
    "            110,\n",
    "            111,\n",
    "            112,\n",
    "            113,\n",
    "            114,\n",
    "            115,\n",
    "            116,\n",
    "            117,\n",
    "            118,\n",
    "            119,\n",
    "            120,\n",
    "            121,\n",
    "            122,\n",
    "            123,\n",
    "            124,\n",
    "            125,\n",
    "            126,\n",
    "            127,\n",
    "            128,\n",
    "            129,\n",
    "            130,\n",
    "            131,\n",
    "            132,\n",
    "            133,\n",
    "            134,\n",
    "            135,\n",
    "            136,\n",
    "            137,\n",
    "            138,\n",
    "            139,\n",
    "            140,\n",
    "            141,\n",
    "            142,\n",
    "            143,\n",
    "            144,\n",
    "            145,\n",
    "            146,\n",
    "            147,\n",
    "            148,\n",
    "            149,\n",
    "            150,\n",
    "            151,\n",
    "            152,\n",
    "            153,\n",
    "            154,\n",
    "            155,\n",
    "            156,\n",
    "            157,\n",
    "            158,\n",
    "            159,\n",
    "            160,\n",
    "            161,\n",
    "            162,\n",
    "            163,\n",
    "            164,\n",
    "            165,\n",
    "            166,\n",
    "            167,\n",
    "            168,\n",
    "            169,\n",
    "            170,\n",
    "            171,\n",
    "            172,\n",
    "            173,\n",
    "            174,\n",
    "            175,\n",
    "            176,\n",
    "            177,\n",
    "            178,\n",
    "            179,\n",
    "            180,\n",
    "            181,\n",
    "            182,\n",
    "            183,\n",
    "            184,\n",
    "            185,\n",
    "            186,\n",
    "            187,\n",
    "            188,\n",
    "            189,\n",
    "            190,\n",
    "            191,\n",
    "            192,\n",
    "            193,\n",
    "            194,\n",
    "            195,\n",
    "            196,\n",
    "            197,\n",
    "            198,\n",
    "            199,\n",
    "            200,\n",
    "            201,\n",
    "            202,\n",
    "            203,\n",
    "            204,\n",
    "            205,\n",
    "            206,\n",
    "            207,\n",
    "            208,\n",
    "            209,\n",
    "            210,\n",
    "            211,\n",
    "            212,\n",
    "            213,\n",
    "            214,\n",
    "            215,\n",
    "            216,\n",
    "            217,\n",
    "            218,\n",
    "            219,\n",
    "            220,\n",
    "            221,\n",
    "            222,\n",
    "            223,\n",
    "            224,\n",
    "            225,\n",
    "            226,\n",
    "            227,\n",
    "            228,\n",
    "            229,\n",
    "            230,\n",
    "            231,\n",
    "            232,\n",
    "            233,\n",
    "            234,\n",
    "            235,\n",
    "            236,\n",
    "            237,\n",
    "            238,\n",
    "            239,\n",
    "            240,\n",
    "            241,\n",
    "            242,\n",
    "            243,\n",
    "            244,\n",
    "            245,\n",
    "            246,\n",
    "            247,\n",
    "            248,\n",
    "            249,\n",
    "            250,\n",
    "            251,\n",
    "            252,\n",
    "            253,\n",
    "            254,\n",
    "            255,\n",
    "            256,\n",
    "            257,\n",
    "            258,\n",
    "            259,\n",
    "            260,\n",
    "            261,\n",
    "            262,\n",
    "            263,\n",
    "            264,\n",
    "            265,\n",
    "            266,\n",
    "            267,\n",
    "            268,\n",
    "            269,\n",
    "            270,\n",
    "            271,\n",
    "            272,\n",
    "            273,\n",
    "            274,\n",
    "            275,\n",
    "            276,\n",
    "            277,\n",
    "            278,\n",
    "            279,\n",
    "            280,\n",
    "            281,\n",
    "            282,\n",
    "            283,\n",
    "            284,\n",
    "            285,\n",
    "            286,\n",
    "            287,\n",
    "            288,\n",
    "            289,\n",
    "            290,\n",
    "            291,\n",
    "            292,\n",
    "            293,\n",
    "            294,\n",
    "            295,\n",
    "            296,\n",
    "            297,\n",
    "            298,\n",
    "            299,\n",
    "            300,\n",
    "            301,\n",
    "            302,\n",
    "            303,\n",
    "            304,\n",
    "            305,\n",
    "            306,\n",
    "            307,\n",
    "            308,\n",
    "            309,\n",
    "            310,\n",
    "            311,\n",
    "            312,\n",
    "            313,\n",
    "            314,\n",
    "            315,\n",
    "            316,\n",
    "            317,\n",
    "            318,\n",
    "            319,\n",
    "            320,\n",
    "            321,\n",
    "            322,\n",
    "            323,\n",
    "            324,\n",
    "            325,\n",
    "            326,\n",
    "            327,\n",
    "            328,\n",
    "            329,\n",
    "            330,\n",
    "            331,\n",
    "            332,\n",
    "            333,\n",
    "            334,\n",
    "            335,\n",
    "            336,\n",
    "            337,\n",
    "            338,\n",
    "            339,\n",
    "            340,\n",
    "            341,\n",
    "            342,\n",
    "            343,\n",
    "            344,\n",
    "            345,\n",
    "            346,\n",
    "            347,\n",
    "            348,\n",
    "            349,\n",
    "            350,\n",
    "            351,\n",
    "            352,\n",
    "            353,\n",
    "            354,\n",
    "            355,\n",
    "            356,\n",
    "            357,\n",
    "            358,\n",
    "            359,\n",
    "            360,\n",
    "            361,\n",
    "            362,\n",
    "            363,\n",
    "            364,\n",
    "            365,\n",
    "            366,\n",
    "            367,\n",
    "            368,\n",
    "            369,\n",
    "            370,\n",
    "            371,\n",
    "            372,\n",
    "            373,\n",
    "            374,\n",
    "            375,\n",
    "            376,\n",
    "            377,\n",
    "            378,\n",
    "            379,\n",
    "            380,\n",
    "            381,\n",
    "            382,\n",
    "            383,\n",
    "            384,\n",
    "            385,\n",
    "            386,\n",
    "            387,\n",
    "            388,\n",
    "            389,\n",
    "            390,\n",
    "            391,\n",
    "            392,\n",
    "            393,\n",
    "            394,\n",
    "            395,\n",
    "            396,\n",
    "            397,\n",
    "            398,\n",
    "            399,\n",
    "            400,\n",
    "            401,\n",
    "            402,\n",
    "            403,\n",
    "            404,\n",
    "            405,\n",
    "            406,\n",
    "            407,\n",
    "            408,\n",
    "            409,\n",
    "            410,\n",
    "            411,\n",
    "            412,\n",
    "            413,\n",
    "            414,\n",
    "            415,\n",
    "            416,\n",
    "            417,\n",
    "            418,\n",
    "            419,\n",
    "            420,\n",
    "            421,\n",
    "            422,\n",
    "            423,\n",
    "            424,\n",
    "            425,\n",
    "            426,\n",
    "            427,\n",
    "            428,\n",
    "            429,\n",
    "            430,\n",
    "            431,\n",
    "            432,\n",
    "            433,\n",
    "            434,\n",
    "            435,\n",
    "            436,\n",
    "            437,\n",
    "            438,\n",
    "            439,\n",
    "            440,\n",
    "            441,\n",
    "            442,\n",
    "            443,\n",
    "            444,\n",
    "            445,\n",
    "            446,\n",
    "            447,\n",
    "            448,\n",
    "            449,\n",
    "            450,\n",
    "            451,\n",
    "            452,\n",
    "            453,\n",
    "            454,\n",
    "            455,\n",
    "            456,\n",
    "            457,\n",
    "            458,\n",
    "            459,\n",
    "            460,\n",
    "            461,\n",
    "            462,\n",
    "            463,\n",
    "            464,\n",
    "            465,\n",
    "            466,\n",
    "            467,\n",
    "            468,\n",
    "            469,\n",
    "            470,\n",
    "            471,\n",
    "            472,\n",
    "            473,\n",
    "            474,\n",
    "            475,\n",
    "            476,\n",
    "            477,\n",
    "            478,\n",
    "            479,\n",
    "            480,\n",
    "            481,\n",
    "            482,\n",
    "            483,\n",
    "            484,\n",
    "            485,\n",
    "            486,\n",
    "            487,\n",
    "            488,\n",
    "            489,\n",
    "            490,\n",
    "            491,\n",
    "            492,\n",
    "            493,\n",
    "            494,\n",
    "            495,\n",
    "            496,\n",
    "            497,\n",
    "            498,\n",
    "            499,\n",
    "            500,\n",
    "            501,\n",
    "            502,\n",
    "            503,\n",
    "            504,\n",
    "            505,\n",
    "            506,\n",
    "            507,\n",
    "            508,\n",
    "            509,\n",
    "            510,\n",
    "            511,\n",
    "            512,\n",
    "            513,\n",
    "            514,\n",
    "            515,\n",
    "            516,\n",
    "            517,\n",
    "            518,\n",
    "            519,\n",
    "            520,\n",
    "            521,\n",
    "            522,\n",
    "            523,\n",
    "            524,\n",
    "            525,\n",
    "            526,\n",
    "            527,\n",
    "            528,\n",
    "            529,\n",
    "            530,\n",
    "            531,\n",
    "            532,\n",
    "            533,\n",
    "            534,\n",
    "            535,\n",
    "            536,\n",
    "            537,\n",
    "            538,\n",
    "            539,\n",
    "            540,\n",
    "            541,\n",
    "            542,\n",
    "            543,\n",
    "            544,\n",
    "            545,\n",
    "            546,\n",
    "            547,\n",
    "            548,\n",
    "            549,\n",
    "            550,\n",
    "            551,\n",
    "            552,\n",
    "            553,\n",
    "            554,\n",
    "            555,\n",
    "            556,\n",
    "            557,\n",
    "            558,\n",
    "            559,\n",
    "            560,\n",
    "            561,\n",
    "            562,\n",
    "            563,\n",
    "            564,\n",
    "            565,\n",
    "            566,\n",
    "            567,\n",
    "            568,\n",
    "            569,\n",
    "            570,\n",
    "            571,\n",
    "            572,\n",
    "            573,\n",
    "            574,\n",
    "            575,\n",
    "            576,\n",
    "            577,\n",
    "            578,\n",
    "            579,\n",
    "            580,\n",
    "            581,\n",
    "            582,\n",
    "            583,\n",
    "            584,\n",
    "            585,\n",
    "            586,\n",
    "            587,\n",
    "            588,\n",
    "            589,\n",
    "            590,\n",
    "            591,\n",
    "            592,\n",
    "            593,\n",
    "            594,\n",
    "            595,\n",
    "            596,\n",
    "            597,\n",
    "            598,\n",
    "            599,\n",
    "            600,\n",
    "            601,\n",
    "            602,\n",
    "            603,\n",
    "            604,\n",
    "            605,\n",
    "            606,\n",
    "            607,\n",
    "            608,\n",
    "            609,\n",
    "            610,\n",
    "            611,\n",
    "            612,\n",
    "            613,\n",
    "            614,\n",
    "            615,\n",
    "            616,\n",
    "            617,\n",
    "            618,\n",
    "            619,\n",
    "            620,\n",
    "            621,\n",
    "            622,\n",
    "            623,\n",
    "            624,\n",
    "            625,\n",
    "            626,\n",
    "            627,\n",
    "            628,\n",
    "            629,\n",
    "            630,\n",
    "            631,\n",
    "            632,\n",
    "            633,\n",
    "            634,\n",
    "            635,\n",
    "            636,\n",
    "            637,\n",
    "            638,\n",
    "            639,\n",
    "            640,\n",
    "            641,\n",
    "            642,\n",
    "            643,\n",
    "            644,\n",
    "            645,\n",
    "            646,\n",
    "            647,\n",
    "            648,\n",
    "            649,\n",
    "            650,\n",
    "            651,\n",
    "            652,\n",
    "            653,\n",
    "            654,\n",
    "            655,\n",
    "            656,\n",
    "            657,\n",
    "            658,\n",
    "            659,\n",
    "            660,\n",
    "            661,\n",
    "            662,\n",
    "            663,\n",
    "            664,\n",
    "            665,\n",
    "            666,\n",
    "            667,\n",
    "            668,\n",
    "            669,\n",
    "            670,\n",
    "            671,\n",
    "            672,\n",
    "            673,\n",
    "            674,\n",
    "            675,\n",
    "            676,\n",
    "            677,\n",
    "            678,\n",
    "            679,\n",
    "            680,\n",
    "            681,\n",
    "            682,\n",
    "            683,\n",
    "            684,\n",
    "            685,\n",
    "            686,\n",
    "            687,\n",
    "            688,\n",
    "            689,\n",
    "            690,\n",
    "            691,\n",
    "            692,\n",
    "            693,\n",
    "            694,\n",
    "            695,\n",
    "            696,\n",
    "            697,\n",
    "            698,\n",
    "            699,\n",
    "            700,\n",
    "            701,\n",
    "            702,\n",
    "            703,\n",
    "            704,\n",
    "            705,\n",
    "            706,\n",
    "            707,\n",
    "            708,\n",
    "            709,\n",
    "            710,\n",
    "            711,\n",
    "            712,\n",
    "            713,\n",
    "            714,\n",
    "            715,\n",
    "            716,\n",
    "            717,\n",
    "            718,\n",
    "            719,\n",
    "            720,\n",
    "            721,\n",
    "            722,\n",
    "            723,\n",
    "            724,\n",
    "            725,\n",
    "            726,\n",
    "            727,\n",
    "            728,\n",
    "            729,\n",
    "            730,\n",
    "            731,\n",
    "            732,\n",
    "            733,\n",
    "            734,\n",
    "            735,\n",
    "            736,\n",
    "            737,\n",
    "            738,\n",
    "            739,\n",
    "            740,\n",
    "            741,\n",
    "            742,\n",
    "            743,\n",
    "            744,\n",
    "            745,\n",
    "            746,\n",
    "            747,\n",
    "            748,\n",
    "            749,\n",
    "            750,\n",
    "            751,\n",
    "            752,\n",
    "            753,\n",
    "            754,\n",
    "            755,\n",
    "            756,\n",
    "            757,\n",
    "            758,\n",
    "            759,\n",
    "            760,\n",
    "            761,\n",
    "            762,\n",
    "            763,\n",
    "            764,\n",
    "            765,\n",
    "            766,\n",
    "            767,\n",
    "        ],\n",
    "        [\n",
    "            768,\n",
    "            769,\n",
    "            770,\n",
    "            771,\n",
    "            772,\n",
    "            773,\n",
    "            774,\n",
    "            775,\n",
    "            776,\n",
    "            777,\n",
    "            778,\n",
    "            779,\n",
    "            780,\n",
    "            781,\n",
    "            782,\n",
    "            783,\n",
    "            784,\n",
    "            785,\n",
    "            786,\n",
    "            787,\n",
    "            788,\n",
    "            789,\n",
    "            790,\n",
    "            791,\n",
    "            792,\n",
    "            793,\n",
    "            794,\n",
    "            795,\n",
    "            796,\n",
    "            797,\n",
    "            798,\n",
    "            799,\n",
    "            800,\n",
    "            801,\n",
    "            802,\n",
    "            803,\n",
    "            804,\n",
    "            805,\n",
    "            806,\n",
    "            807,\n",
    "            808,\n",
    "            809,\n",
    "            810,\n",
    "            811,\n",
    "            812,\n",
    "            813,\n",
    "            814,\n",
    "            815,\n",
    "            816,\n",
    "            817,\n",
    "            818,\n",
    "            819,\n",
    "            820,\n",
    "            821,\n",
    "            822,\n",
    "            823,\n",
    "            824,\n",
    "            825,\n",
    "            826,\n",
    "            827,\n",
    "            828,\n",
    "            829,\n",
    "            830,\n",
    "            831,\n",
    "            832,\n",
    "            833,\n",
    "            834,\n",
    "            835,\n",
    "            836,\n",
    "            837,\n",
    "            838,\n",
    "            839,\n",
    "            840,\n",
    "            841,\n",
    "            842,\n",
    "            843,\n",
    "            844,\n",
    "            845,\n",
    "            846,\n",
    "            847,\n",
    "            848,\n",
    "            849,\n",
    "            850,\n",
    "            851,\n",
    "            852,\n",
    "            853,\n",
    "            854,\n",
    "            855,\n",
    "            856,\n",
    "            857,\n",
    "            858,\n",
    "            859,\n",
    "            860,\n",
    "            861,\n",
    "            862,\n",
    "            863,\n",
    "            864,\n",
    "            865,\n",
    "            866,\n",
    "            867,\n",
    "            868,\n",
    "            869,\n",
    "            870,\n",
    "            871,\n",
    "        ],\n",
    "    ],\n",
    "    n_samples_representative=10000,\n",
    "    # verbose=1,\n",
    "    cohirf_kwargs=dict(\n",
    "        # verbose=1,\n",
    "        n_samples_representative=10000,\n",
    "    ),\n",
    ")\n",
    "search_space = dict(\n",
    "        cohirf_kwargs=dict(\n",
    "                n_features=optuna.distributions.FloatDistribution(0.1, 1),\n",
    "                repetitions=optuna.distributions.IntDistribution(2, 10),\n",
    "                kmeans_n_clusters=optuna.distributions.IntDistribution(2, 5),\n",
    "            )\n",
    "        )\n",
    "default_values = [\n",
    "\tdict(\n",
    "        cohirf_kwargs=dict(\n",
    "            n_features=0.3,\n",
    "            repetitions=5,\n",
    "            kmeans_n_clusters=3,\n",
    "        )\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0ffa759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26651881532943baafb2f35a1322a4f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Combinations completed:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/19 13:20:14 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/19 13:20:14 INFO mlflow.store.db.utils: Updating database tables\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "INFO  [alembic.runtime.migration] Running upgrade  -> 451aebb31d03, add metric step\n",
      "INFO  [alembic.runtime.migration] Running upgrade 451aebb31d03 -> 90e64c465722, migrate user column to tags\n",
      "INFO  [alembic.runtime.migration] Running upgrade 90e64c465722 -> 181f10493468, allow nulls for metric values\n",
      "INFO  [alembic.runtime.migration] Running upgrade 181f10493468 -> df50e92ffc5e, Add Experiment Tags Table\n",
      "INFO  [alembic.runtime.migration] Running upgrade df50e92ffc5e -> 7ac759974ad8, Update run tags with larger limit\n",
      "INFO  [alembic.runtime.migration] Running upgrade 7ac759974ad8 -> 89d4b8295536, create latest metrics table\n",
      "INFO  [89d4b8295536_create_latest_metrics_table_py] Migration complete!\n",
      "INFO  [alembic.runtime.migration] Running upgrade 89d4b8295536 -> 2b4d017a5e9b, add model registry tables to db\n",
      "INFO  [2b4d017a5e9b_add_model_registry_tables_to_db_py] Adding registered_models and model_versions tables to database.\n",
      "INFO  [2b4d017a5e9b_add_model_registry_tables_to_db_py] Migration complete!\n",
      "INFO  [alembic.runtime.migration] Running upgrade 2b4d017a5e9b -> cfd24bdc0731, Update run status constraint with killed\n",
      "INFO  [alembic.runtime.migration] Running upgrade cfd24bdc0731 -> 0a8213491aaa, drop_duplicate_killed_constraint\n",
      "INFO  [alembic.runtime.migration] Running upgrade 0a8213491aaa -> 728d730b5ebd, add registered model tags table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 728d730b5ebd -> 27a6a02d2cf1, add model version tags table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 27a6a02d2cf1 -> 84291f40a231, add run_link to model_version\n",
      "INFO  [alembic.runtime.migration] Running upgrade 84291f40a231 -> a8c4a736bde6, allow nulls for run_id\n",
      "INFO  [alembic.runtime.migration] Running upgrade a8c4a736bde6 -> 39d1c3be5f05, add_is_nan_constraint_for_metrics_tables_if_necessary\n",
      "INFO  [alembic.runtime.migration] Running upgrade 39d1c3be5f05 -> c48cb773bb87, reset_default_value_for_is_nan_in_metrics_table_for_mysql\n",
      "INFO  [alembic.runtime.migration] Running upgrade c48cb773bb87 -> bd07f7e963c5, create index on run_uuid\n",
      "INFO  [alembic.runtime.migration] Running upgrade bd07f7e963c5 -> 0c779009ac13, add deleted_time field to runs table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 0c779009ac13 -> cc1f77228345, change param value length to 500\n",
      "INFO  [alembic.runtime.migration] Running upgrade cc1f77228345 -> 97727af70f4d, Add creation_time and last_update_time to experiments table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 97727af70f4d -> 3500859a5d39, Add Model Aliases table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 3500859a5d39 -> 7f2a7d5fae7d, add datasets inputs input_tags tables\n",
      "INFO  [alembic.runtime.migration] Running upgrade 7f2a7d5fae7d -> 2d6e25af4d3e, increase max param val length from 500 to 8000\n",
      "INFO  [alembic.runtime.migration] Running upgrade 2d6e25af4d3e -> acf3f17fdcc7, add storage location field to model versions\n",
      "INFO  [alembic.runtime.migration] Running upgrade acf3f17fdcc7 -> 867495a8f9d4, add trace tables\n",
      "INFO  [alembic.runtime.migration] Running upgrade 867495a8f9d4 -> 5b0e9adcef9c, add cascade deletion to trace tables foreign keys\n",
      "INFO  [alembic.runtime.migration] Running upgrade 5b0e9adcef9c -> 4465047574b1, increase max dataset schema size\n",
      "INFO  [alembic.runtime.migration] Running upgrade 4465047574b1 -> f5a4f2784254, increase run tag value limit to 8000\n",
      "INFO  [alembic.runtime.migration] Running upgrade f5a4f2784254 -> 0584bdc529eb, add cascading deletion to datasets from experiments\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b52f3e4e6134150b4473277921475c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Trials:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARI: 0.2252, HPO time: 1661.96s, Best time: 102.68s\n"
     ]
    }
   ],
   "source": [
    "experiment = HPOCSVClusteringExperiment(\n",
    "    # hpo\n",
    "    n_trials=20,\n",
    "    hpo_seed=0,\n",
    "    hpo_metric=\"adjusted_rand\",\n",
    "    direction=\"maximize\",\n",
    "    # model\n",
    "    experiment_name=\"test-wine\",\n",
    "    model=model,\n",
    "    model_params=model_params,\n",
    "\tsearch_space=search_space,\n",
    "\tdefault_values=default_values,\n",
    "    seed_model=0,\n",
    "\tn_jobs=2,\n",
    "    # dataset\n",
    "    dataset_name=\"wine\",\n",
    "    raise_on_error=True,\n",
    "    verbose=1,\n",
    "    **experiment_params,\n",
    ")\n",
    "result = experiment.run(return_results=True)[0]\n",
    "ari = result[\"evaluate_model_return\"][\"best/adjusted_rand\"]\n",
    "hpo_time = result[\"fit_model_return\"][\"elapsed_time\"]\n",
    "best_time = result[\"evaluate_model_return\"][\"best/elapsed_time\"]\n",
    "print(f\"ARI: {ari:.4f}, HPO time: {hpo_time:.2f}s, Best time: {best_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6efc7805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c81a56ffac0e421b93b2325a61804ab0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Combinations completed:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0126548585424d50bf5d672c8d95514d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Trials:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARI: 0.2448, HPO time: 1973.98s, Best time: 94.84s\n"
     ]
    }
   ],
   "source": [
    "experiment = HPOCSVClusteringExperiment(\n",
    "    # hpo\n",
    "    n_trials=20,\n",
    "    hpo_seed=0,\n",
    "    hpo_metric=\"adjusted_rand\",\n",
    "    direction=\"maximize\",\n",
    "    # model\n",
    "    experiment_name=\"test-wine\",\n",
    "    model=model,\n",
    "    model_params=model_params,\n",
    "    search_space=search_space,\n",
    "    default_values=default_values,\n",
    "    seed_model=0,\n",
    "    n_jobs=2,\n",
    "    # dataset\n",
    "    dataset_name=\"wine\",\n",
    "    raise_on_error=True,\n",
    "    verbose=1,\n",
    "    **experiment_params,\n",
    ")\n",
    "result = experiment.run(return_results=True)[0]\n",
    "ari = result[\"evaluate_model_return\"][\"best/adjusted_rand\"]\n",
    "hpo_time = result[\"fit_model_return\"][\"elapsed_time\"]\n",
    "best_time = result[\"evaluate_model_return\"][\"best/elapsed_time\"]\n",
    "print(f\"ARI: {ari:.4f}, HPO time: {hpo_time:.2f}s, Best time: {best_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "276660eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183.94602207247522"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"evaluate_model_return\"][\"best/calinski_harabasz_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9ce7a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLURM_JOB_ID: 77521\n",
      "2025-08-19 14:34:01\n",
      "Starting experiment...\n",
      "combination_names: ['model', 'seed_model', 'dataset_name']\n",
      "combinations: [('KMeans', 0, 'wine')]\n",
      "unique_params: {'timeout_fit': None, 'timeout_combination': None, 'n_jobs': 1, 'model_params': {}, 'max_threads': None, 'calculate_davies_bouldin': False, 'calculate_full_silhouette': False, 'calculate_metrics_even_if_too_many_clusters': False, 'standardize': False, 'hpo_framework': 'optuna', 'n_trials': 20, 'timeout_hpo': 0, 'timeout_trial': 0, 'max_concurrent_trials': 1, 'hpo_seed': 0, 'sampler': 'tpe', 'pruner': 'none', 'direction': 'maximize', 'hpo_metric': 'calinski_harabasz_score'}\n",
      "extra_params: {}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f60ea73e39854f1f987331ffdbe90a68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Combinations completed:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLURM_JOB_ID: 77521\n",
      "2025-08-19 14:34:01\n",
      "Running...\n",
      "model: KMeans\n",
      "seed_model: 0\n",
      "dataset_name: wine\n",
      "timeout_fit: None\n",
      "timeout_combination: None\n",
      "n_jobs: 1\n",
      "model_params: {}\n",
      "max_threads: None\n",
      "calculate_davies_bouldin: False\n",
      "calculate_full_silhouette: False\n",
      "calculate_metrics_even_if_too_many_clusters: False\n",
      "standardize: False\n",
      "hpo_framework: optuna\n",
      "n_trials: 20\n",
      "timeout_hpo: 0\n",
      "timeout_trial: 0\n",
      "max_concurrent_trials: 1\n",
      "hpo_seed: 0\n",
      "sampler: tpe\n",
      "pruner: none\n",
      "direction: maximize\n",
      "hpo_metric: calinski_harabasz_score\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "568ea10d9e564a8db3edc13846afb63a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Trials:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt/nfs)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLURM_JOB_ID: 77521\n",
      "2025-08-19 14:43:54\n",
      "Finished!\n",
      "total_elapsed_time: 592.0910736676306\n",
      "model: KMeans\n",
      "seed_model: 0\n",
      "dataset_name: wine\n",
      "timeout_fit: None\n",
      "timeout_combination: None\n",
      "n_jobs: 1\n",
      "model_params: {'n_clusters': 9}\n",
      "max_threads: None\n",
      "calculate_davies_bouldin: False\n",
      "calculate_full_silhouette: False\n",
      "calculate_metrics_even_if_too_many_clusters: False\n",
      "standardize: False\n",
      "hpo_framework: optuna\n",
      "n_trials: 20\n",
      "timeout_hpo: 0\n",
      "timeout_trial: 0\n",
      "max_concurrent_trials: 1\n",
      "hpo_seed: 0\n",
      "sampler: tpe\n",
      "pruner: none\n",
      "direction: maximize\n",
      "hpo_metric: calinski_harabasz_score\n",
      "\n",
      "SLURM_JOB_ID: 77521\n",
      "2025-08-19 14:43:54\n",
      "Combinations completed:   0%|          | 0/1 [09:52<?, ?it/s]\n",
      "succesfully_completed: 1\n",
      "failed: 0\n",
      "none: 0\n",
      "\n",
      "ARI: 51206.0039, HPO time: 567.65s, Best time: 1.37s\n"
     ]
    }
   ],
   "source": [
    "experiment = HPOCSVClusteringExperiment(\n",
    "    # hpo\n",
    "    n_trials=20,\n",
    "    hpo_seed=0,\n",
    "    hpo_metric=\"calinski_harabasz_score\",\n",
    "    direction=\"maximize\",\n",
    "    # model\n",
    "    experiment_name=\"test-wine\",\n",
    "    model=\"KMeans\",\n",
    "    seed_model=0,\n",
    "    # dataset\n",
    "    dataset_name=\"wine\",\n",
    "    raise_on_error=True,\n",
    "    **experiment_params,\n",
    ")\n",
    "result = experiment.run(return_results=True)[0]\n",
    "ari = result[\"evaluate_model_return\"][\"best/calinski_harabasz_score\"]\n",
    "hpo_time = result[\"fit_model_return\"][\"elapsed_time\"]\n",
    "best_time = result[\"evaluate_model_return\"][\"best/elapsed_time\"]\n",
    "print(f\"ARI: {ari:.4f}, HPO time: {hpo_time:.2f}s, Best time: {best_time:.2f}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cohirf",
   "language": "python",
   "name": "cohirf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
