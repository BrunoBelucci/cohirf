{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-30T18:33:22.485059Z",
     "start_time": "2025-03-30T18:33:21.683834Z"
    }
   },
   "source": [
    "from recursive_clustering.experiment.tested_models import models_dict\n",
    "from pathlib import Path\n",
    "from itertools import product"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T18:33:22.496697Z",
     "start_time": "2025-03-30T18:33:22.490898Z"
    }
   },
   "cell_type": "code",
   "source": [
    "models_nicknames = list(models_dict.keys())\n",
    "script_dir = Path() / 'scripts'\n",
    "script_dir.mkdir(parents=True, exist_ok=True)\n",
    "models_nicknames.remove('WBMS')\n",
    "models_nicknames.sort()\n",
    "models_nicknames"
   ],
   "id": "3f98c393bb9116ba",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AffinityPropagation',\n",
       " 'AverageAgglomerativeClustering',\n",
       " 'Clique',\n",
       " 'CompleteAgglomerativeClustering',\n",
       " 'DBSCAN',\n",
       " 'HDBSCAN',\n",
       " 'IRFLLRR',\n",
       " 'KMeans',\n",
       " 'KMeansProj',\n",
       " 'MeanShift',\n",
       " 'OPTICS',\n",
       " 'Proclus',\n",
       " 'RecursiveClustering',\n",
       " 'RecursiveClusteringHDBSCAN',\n",
       " 'RecursiveClusteringHDBSCAN_full',\n",
       " 'RecursiveClusteringSCSRGF',\n",
       " 'RecursiveClusteringSCSRGF_full',\n",
       " 'RecursiveClustering_full',\n",
       " 'SingleAgglomerativeClustering',\n",
       " 'SpectralClustering',\n",
       " 'SpectralSubspaceRandomization',\n",
       " 'WardAgglomerativeClustering']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T18:33:22.579722Z",
     "start_time": "2025-03-30T18:33:22.576739Z"
    }
   },
   "cell_type": "code",
   "source": "len(models_nicknames)",
   "id": "5168dc77fd183100",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T18:33:22.625648Z",
     "start_time": "2025-03-30T18:33:22.622636Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # sensible parameters for n_classes=5\n",
    "# models_params = {\n",
    "#     'AffinityPropagation': {\"damping\": 0.9},\n",
    "#     'AverageAgglomerativeClustering': {'n_clusters': 5},\n",
    "#     'Clique': {},\n",
    "#     'CompleteAgglomerativeClustering': {'n_clusters': 5},\n",
    "#     'DBSCAN': {},\n",
    "#     'HDBSCAN': {},\n",
    "#     'IRFLLRR': {'sc_n_clusters': 5},\n",
    "#     'KMeans': {'n_clusters': 5},\n",
    "#     'KMeansProj': {},\n",
    "#     'MeanShift': {},\n",
    "#     'OPTICS': {},\n",
    "#     'Proclus': {'n_clusters': 5},\n",
    "#     'RecursiveClustering': {},\n",
    "#     'SingleAgglomerativeClustering': {'n_clusters': 5},\n",
    "#     'SpectralClustering': {'n_clusters': 5},\n",
    "#     'SpectralSubspaceRandomization': {'sc_n_clusters': 5},\n",
    "#     'WardAgglomerativeClustering': {'n_clusters': 5},\n",
    "# }"
   ],
   "id": "292e1134d4d9243e",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T18:33:22.671951Z",
     "start_time": "2025-03-30T18:33:22.668042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "models_samplers = {\n",
    "    'AffinityPropagation': 'tpe',\n",
    "    'AverageAgglomerativeClustering': 'grid',\n",
    "    'Clique': 'tpe',\n",
    "    'CompleteAgglomerativeClustering': 'grid',\n",
    "    'DBSCAN': 'tpe',\n",
    "    'HDBSCAN': 'grid',\n",
    "    'IRFLLRR': 'tpe',\n",
    "    'KMeans': 'grid',\n",
    "    'KMeansProj': 'grid',\n",
    "    'MeanShift': 'grid',\n",
    "    'OPTICS': 'grid',\n",
    "    'Proclus': 'tpe',\n",
    "    'RecursiveClustering': 'tpe',\n",
    "    'RecursiveClustering_full': 'tpe',\n",
    "    'SingleAgglomerativeClustering': 'grid',\n",
    "    'SpectralClustering': 'grid',\n",
    "    'SpectralSubspaceRandomization': 'tpe',\n",
    "    'WardAgglomerativeClustering': 'grid',\n",
    "}"
   ],
   "id": "a05b181d24bf81c9",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T18:46:33.185681Z",
     "start_time": "2025-03-30T18:46:33.163426Z"
    }
   },
   "cell_type": "code",
   "source": [
    "models_nicknames = ['RecursiveClustering', 'KMeans']\n",
    "n_samples = \"1000\"\n",
    "n_informative_features = \"1000\"\n",
    "n_random_features = \"0,100,500,1000,2300,9000\"  # aprox 0, 0.1, 0.3, 0.5, 0.7, 0.9\n",
    "n_features = \"\"\n",
    "pct_random_features = \"\"\n",
    "# pct_random_features = \"0.3\"\n",
    "n_centers = \"5\"\n",
    "distances = \"50\"\n",
    "seeds_unified = \"0-1-2-3-4\"\n",
    "# seeds_unified = \"3\"\n",
    "direction = 'maximize'\n",
    "# hpo_metric = 'silhouette'\n",
    "hpo_metric = 'silhouette'\n",
    "n_trials = '30'\n",
    "pruner = 'none'\n",
    "timeout_hpo = 7200\n",
    "timeout_trial = 600\n",
    "suffix = f'{hpo_metric}'\n",
    "c = 40\n",
    "# dask\n",
    "# dask_cluster_type = \"local\"\n",
    "# n_workers = 20\n",
    "# n_cores_per_worker = 4\n",
    "# n_processes_per_worker = 1\n",
    "# n_threads_per_worker = 4\n",
    "# n_cores_per_task = 4\n",
    "# n_processes_per_task = 1\n",
    "# n_threads_per_task = 4\n",
    "# dask_memory= '8GB'\n",
    "dask_cluster_type = 'local'\n",
    "n_workers = '10'\n",
    "n_cores_per_worker = '4'\n",
    "n_processes_per_worker = '1'\n",
    "n_threads_per_worker = '8'\n",
    "n_cores_per_task = '4'\n",
    "n_processes_per_task = '1'\n",
    "n_threads_per_task = '8'\n",
    "dask_memory= '12GB'\n",
    "# calculate job arrays\n",
    "combinations = product(n_samples.replace(',','-').split('-'), n_features.replace(',','-').split('-'), seeds_unified.replace(',','-').split('-'), pct_random_features.replace(',','-').split('-'), distances.replace(',','-').split('-'), n_random_features.replace(',','-').split('-'), n_informative_features.replace(',','-').split('-'))\n",
    "n_combinations = 6\n",
    "array_start = 0\n",
    "array_end = int(n_combinations - 1)\n",
    "array_step = 1\n",
    "array_simultaneous = min(10, int(n_combinations))\n",
    "array_batch = array_simultaneous\n",
    "array_fist_start = 0\n",
    "array_fist_end = int(array_simultaneous - 1)\n",
    "for model in models_nicknames:\n",
    "    sampler = models_samplers[model]\n",
    "    if model.find('RecursiveClustering') != -1:\n",
    "        # we will use the threads of the main process to accelerate\n",
    "        n_jobs = 1\n",
    "    else:\n",
    "        n_jobs = 2\n",
    "    file = script_dir / f'gaussian_random_{model}_{suffix}.sbatch'\n",
    "    file_content=f\"\"\"#!/bin/bash\n",
    "#SBATCH --job-name={file.name[:-len('.sbatch')]}\n",
    "#SBATCH -c {c}\n",
    "#SBATCH --exclude=clust[6-12]\n",
    "#SBATCH --output=/home/users/belucci/recursive_clustering/results/sbatch_outputs/%x-%A_%a.out\n",
    "#SBATCH --error=/home/users/belucci/recursive_clustering/results/sbatch_errors/%x-%A_%a.err\n",
    "#SBATCH --time=1-00:00:00\n",
    "#SBATCH --array={array_fist_start}-{array_fist_end}:{array_step}%{array_simultaneous}\n",
    "\n",
    "# export OMP_NUM_THREADS=4\n",
    "# export MKL_NUM_THREADS=4\n",
    "# export OPENBLAS_NUM_THREADS=4\n",
    "\n",
    "# we are limited at 300 submitted jobs, so we will submit small array jobs and the last array will resubmit the next array job\n",
    "array_start={array_start}\n",
    "array_end={array_end}\n",
    "array_step={array_step}\n",
    "array_simultaneous={array_simultaneous}\n",
    "array_batch={array_batch}\n",
    "script_path=\"/home/users/belucci/recursive_clustering/run_scripts/gaussian_random/gaussian_random/${{SLURM_JOB_NAME}}.sbatch\"\n",
    "\n",
    "\n",
    "# 1 model, 64 datasets, 10 fold -> 640 models, but slurm is configured to accept at maximum 1001 (array from 0-1000)\n",
    "# then copy another template or modify this one accordingly\n",
    "\n",
    "experiment_root_dir=\"/home/users/belucci/recursive_clustering/results/gaussian_random\"\n",
    "experiment_work_root_dir=\"/tmp/${{SLURM_JOB_NAME}}\"\n",
    "# experiment_save_root_dir=\"\"\n",
    "\n",
    "# For job step/array\n",
    "srun_output=\"${{experiment_root_dir}}/sbatch_output/srun-%x-%A_%a.out\"\n",
    "srun_error=\"${{experiment_root_dir}}/sbatch_error/srun-%x-%A_%a.err\"\n",
    "# dbs_dir=\"${{experiment_work_root_dir}}/dbs\"\n",
    "# mkdir -p $dbs_dir\n",
    "\n",
    "# then copy another template or modify this one accordingly\n",
    "environment_name=\"cohirf\"\n",
    "experiment_python_location=\"/home/users/belucci/recursive_clustering/recursive_clustering/experiment/hpo_gaussian_clustering_experiment.py\"\n",
    "\n",
    "# Create a dictionary with argument names and values\n",
    "declare -A args_dict=(\n",
    "# base\n",
    "[\"models_nickname\"]=\"{model}\"\n",
    "[\"seeds_models\"]=\"\"\n",
    "[\"experiment_name\"]=\"${{SLURM_JOB_NAME}}\"\n",
    "[\"n_jobs\"]=\"{n_jobs}\"\n",
    "[\"models_params\"]=''\n",
    "[\"fits_params\"]=\"\"\n",
    "[\"error_score\"]=\"\"\n",
    "[\"timeout_fit\"]=\"\"\n",
    "[\"timeout_combination\"]=\"\"\n",
    "[\"log_dir\"]=\"${{experiment_root_dir}}/logs\"\n",
    "#[\"log_file_name\"]=\"\"\n",
    "[\"work_root_dir\"]=\"${{experiment_work_root_dir}}/work\"\n",
    "[\"save_root_dir\"]=\"\"\n",
    "[\"mlflow_tracking_uri\"]=\"http://clust9.ceremade.dauphine.lan:5002/\"\n",
    "[\"dask_cluster_type\"]=\"{dask_cluster_type}\"\n",
    "[\"n_workers\"]=\"{n_workers}\"\n",
    "[\"n_cores_per_worker\"]=\"{n_cores_per_worker}\"\n",
    "[\"n_processes_per_worker\"]=\"{n_processes_per_worker}\"\n",
    "[\"n_threads_per_worker\"]=\"{n_threads_per_worker}\"\n",
    "[\"n_cores_per_task\"]=\"{n_cores_per_task}\"\n",
    "[\"n_processes_per_task\"]=\"{n_processes_per_task}\"\n",
    "[\"n_threads_per_task\"]=\"{n_threads_per_task}\"\n",
    "[\"dask_memory\"]=\"{dask_memory}\"\n",
    "[\"dask_job_extra_directives\"]=\"\"\n",
    "[\"dask_address\"]=\"\"\n",
    "[\"n_gpus_per_worker\"]=\"\"\n",
    "[\"n_gpus_per_task\"]=\"\"\n",
    "# hpo\n",
    "[\"hpo_framework\"]=\"\"\n",
    "[\"n_trials\"]=\"{n_trials}\"\n",
    "[\"timeout_hpo\"]=\"{timeout_hpo}\"\n",
    "[\"timeout_trial\"]=\"{timeout_trial}\"\n",
    "[\"max_concurrent_trials\"]=\"\"\n",
    "[\"sampler\"]=\"{sampler}\"\n",
    "[\"pruner\"]=\"{pruner}\"\n",
    "[\"direction\"]=\"{direction}\"\n",
    "#[\"hpo_metric\"]=\"{hpo_metric}\"\n",
    "\n",
    "# gaussian clustering\n",
    ")\n",
    "\n",
    "declare -A bool_args_dict=(\n",
    "# base\n",
    "[\"create_validation_set\"]=0\n",
    "[\"do_not_clean_work_dir\"]=0\n",
    "[\"do_not_log_to_mlflow\"]=0\n",
    "[\"do_not_check_if_exists\"]=0\n",
    "[\"do_not_retry_on_oom\"]=0\n",
    "[\"raise_on_fit_error\"]=0\n",
    ")\n",
    "\n",
    "declare -A array_args_dict=(\n",
    "# Note that bash does not allow arrays inside dictionaries, so we will use strings with ',' as separators and '-' when\n",
    "# we want multiple values for the same argument (basically they will be replaced by ' ')\n",
    "# base\n",
    "[\"n_samples\"]=\"{n_samples}\"\n",
    "# [\"n_features\"]=\"{n_features}\"\n",
    "# [\"pct_random_features\"]=\"{pct_random_features}\"\n",
    "[\"n_random_features\"]=\"{n_random_features}\"\n",
    "[\"n_informative_features\"]=\"{n_informative_features}\"\n",
    "[\"n_centers\"]=\"{n_centers}\"\n",
    "[\"distances\"]=\"{distances}\"\n",
    "[\"seeds_unified\"]=\"{seeds_unified}\"\n",
    "[\"hpo_metric\"]=\"{hpo_metric}\"\n",
    "# in this example we are expecting the combinations Model1+0-1, Model1+2-3, Model1+4, Model2+0-1, Model2+2-3, Model2+4\n",
    ")\n",
    "\n",
    "# bash does not necessarily keep the order of the keys in the dictionary, so we will specify the order here\n",
    "# declare -a array_args_dict_order=(\"n_samples\" \"n_random_features\" \"n_informative_features\" \"n_centers\" \"distances\" \"seeds_unified\" \"hpo_metric\" \"n_features\" \"pct_random_features\")\n",
    "declare -a array_args_dict_order=(\"n_samples\" \"n_random_features\" \"n_informative_features\" \"n_centers\" \"distances\" \"seeds_unified\" \"hpo_metric\")\n",
    "\n",
    "# Construct the argument string\n",
    "args_str=\"\"\n",
    "for key in \"${{!args_dict[@]}}\"; do\n",
    "  if [ -n \"${{args_dict[$key]}}\" ]; then\n",
    "    args_str=\"$args_str --$key ${{args_dict[$key]}}\"\n",
    "  fi\n",
    "done\n",
    "\n",
    "# Add arguments strings that are boolean\n",
    "for key in \"${{!bool_args_dict[@]}}\"; do\n",
    "  if [ \"${{bool_args_dict[$key]}}\" -eq 1 ]; then\n",
    "    args_str=\"${{args_str}} --${{key}}\"\n",
    "  fi\n",
    "done\n",
    "\n",
    "# Construct the cartesian product of the arrays\n",
    "# the idea is to create a string like {{Model1,Model2}}+{{0-1,2-3,4}} and then evaluate it to get the cartesian product\n",
    "# using bash's brace expansion\n",
    "string_for_cartesian_product=\"\"\n",
    "for key in \"${{array_args_dict_order[@]}}\"; do\n",
    "  str_array=${{array_args_dict[$key]}}\n",
    "  n_elements=$(echo $str_array | tr ',' ' ' | wc -w)\n",
    "  if [ ${{n_elements}} -eq 0 ]; then\n",
    "    continue\n",
    "  elif [ ${{n_elements}} -eq 1 ]; then\n",
    "    string_for_cartesian_product=\"${{string_for_cartesian_product}}+${{str_array}}\"\n",
    "  else\n",
    "    string_for_cartesian_product=\"${{string_for_cartesian_product}}+{{${{str_array}}}}\"\n",
    "  fi\n",
    "done\n",
    "\n",
    "# Remove the first '+' character\n",
    "string_for_cartesian_product=${{string_for_cartesian_product:1}}\n",
    "\n",
    "# Evaluate the string to get the cartesian product\n",
    "cartesian_product=$(eval echo $string_for_cartesian_product)\n",
    "\n",
    "# Split the string into an array (1 combination per element)\n",
    "IFS=' ' read -r -a cartesian_product <<< \"${{cartesian_product}}\"\n",
    "# cartesian_product is now an array like [\"Model1+0-1\", \"Model1+2-3\", \"Model1+4\", \"Model2+0-1\", \"Model2+2-3\", \"Model2+4\"]\n",
    "\n",
    "# Activate the conda environment\n",
    "eval \"$(conda shell.bash hook)\"\n",
    "conda activate $environment_name\n",
    "\n",
    "# alternatively we could also run an array job and execute the combination given by the SLURM_ARRAY_TASK_ID\n",
    "i_combination=$SLURM_ARRAY_TASK_ID\n",
    "if [ \"${{i_combination}}\" -ge \"${{#cartesian_product[@]}}\" ]; then\n",
    "  echo \"SLURM_ARRAY_TASK_ID is greater than the number of combinations\"\n",
    "  exit 1\n",
    "fi\n",
    "string_combination=\"\"\n",
    "IFS='+' read -r -a combination <<< \"${{cartesian_product[$i_combination]}}\"\n",
    "i_arg_name=0\n",
    "for key in \"${{array_args_dict_order[@]}}\"; do\n",
    "  value=${{combination[$i_arg_name]//[-]/ }}  # replace '-' by ' '\n",
    "  string_combination=\"${{string_combination}} --${{key}} ${{value}}\"\n",
    "  i_arg_name=$((i_arg_name+1))\n",
    "done\n",
    "# mlflow_tracking_uri=\"sqlite:////${{dbs_dir}}/${{cartesian_product[$i_combination]}}.db\"\n",
    "log_file_name=\"${{SLURM_JOB_NAME}}-${{cartesian_product[$i_combination]}}\"\n",
    "echo \"Running: srun --exclusive -n 1 -c ${{SLURM_CPUS_PER_TASK}} --output=${{srun_output}} --error=${{srun_error}} python ${{experiment_python_location}} ${{args_str}} ${{string_combination}} --log_file_name ${{log_file_name}}\"\n",
    "srun --exclusive -n 1 -c ${{SLURM_CPUS_PER_TASK}} --output=${{srun_output}} --error=${{srun_error}} python ${{experiment_python_location}} ${{args_str}} ${{string_combination}} --log_file_name ${{log_file_name}}\n",
    "wait\n",
    "\n",
    "# If we are at the last array task, submit the next array job\n",
    "if [ \"${{SLURM_ARRAY_TASK_ID}}\" -eq \"${{SLURM_ARRAY_TASK_MAX}}\" ]; then\n",
    "  next_array_start=$(($SLURM_ARRAY_TASK_MIN+array_batch))\n",
    "  # next_array_end will be the minimum between array_end and next_array_start+array_batch\n",
    "  next_array_end=$(( array_end < next_array_start+array_batch ? array_end : next_array_start+array_batch ))\n",
    "  # if next_array_start <= array_end, then we submit job, otherwise we are done\n",
    "  if [ \"${{next_array_start}}\" -le \"${{next_array_end}}\" ]; then\n",
    "    sbatch --array=${{next_array_start}}-${{next_array_end}}:${{array_step}}%${{array_simultaneous}} ${{script_path}}\n",
    "  fi\n",
    "fi\n",
    "\"\"\"\n",
    "    file.write_text(file_content)\n"
   ],
   "id": "86ce89ff1abd7aef",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a26dca0b1500a4ba"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cohirf",
   "language": "python",
   "name": "cohirf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
