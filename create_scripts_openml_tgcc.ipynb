{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-29T10:53:32.901238Z",
     "start_time": "2025-03-29T10:53:32.057113Z"
    }
   },
   "source": [
    "from recursive_clustering.experiment.tested_models import models_dict\n",
    "from pathlib import Path\n",
    "from itertools import product"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T10:53:32.916045Z",
     "start_time": "2025-03-29T10:53:32.909657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "models_nicknames = list(models_dict.keys())\n",
    "script_dir = Path() / 'scripts'\n",
    "script_dir.mkdir(parents=True, exist_ok=True)\n",
    "models_nicknames.remove('WBMS')\n",
    "models_nicknames.sort()\n",
    "models_nicknames"
   ],
   "id": "3f98c393bb9116ba",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AffinityPropagation',\n",
       " 'AverageAgglomerativeClustering',\n",
       " 'Clique',\n",
       " 'CompleteAgglomerativeClustering',\n",
       " 'DBSCAN',\n",
       " 'HDBSCAN',\n",
       " 'IRFLLRR',\n",
       " 'KMeans',\n",
       " 'KMeansProj',\n",
       " 'MeanShift',\n",
       " 'OPTICS',\n",
       " 'Proclus',\n",
       " 'RecursiveClustering',\n",
       " 'RecursiveClustering_full',\n",
       " 'SingleAgglomerativeClustering',\n",
       " 'SpectralClustering',\n",
       " 'SpectralSubspaceRandomization',\n",
       " 'WardAgglomerativeClustering']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T10:53:33.028145Z",
     "start_time": "2025-03-29T10:53:33.024441Z"
    }
   },
   "cell_type": "code",
   "source": "len(models_nicknames)",
   "id": "5168dc77fd183100",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T10:53:33.665967Z",
     "start_time": "2025-03-29T10:53:33.660447Z"
    }
   },
   "cell_type": "code",
   "source": [
    "models_samplers = {\n",
    "    'AffinityPropagation': 'tpe',\n",
    "    'AverageAgglomerativeClustering': 'grid',\n",
    "    'Clique': 'tpe',\n",
    "    'CompleteAgglomerativeClustering': 'grid',\n",
    "    'DBSCAN': 'tpe',\n",
    "    'HDBSCAN': 'grid',\n",
    "    'IRFLLRR': 'tpe',\n",
    "    'KMeans': 'grid',\n",
    "    'KMeansProj': 'grid',\n",
    "    'MeanShift': 'grid',\n",
    "    'OPTICS': 'grid',\n",
    "    'Proclus': 'tpe',\n",
    "    'RecursiveClustering': 'tpe',\n",
    "    'RecursiveClustering_full': 'tpe',\n",
    "    'SingleAgglomerativeClustering': 'grid',\n",
    "    'SpectralClustering': 'grid',\n",
    "    'SpectralSubspaceRandomization': 'tpe',\n",
    "    'WardAgglomerativeClustering': 'grid',\n",
    "}"
   ],
   "id": "a05b181d24bf81c9",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T15:49:29.087769Z",
     "start_time": "2025-03-29T15:49:28.946313Z"
    }
   },
   "cell_type": "code",
   "source": "3600/2",
   "id": "85103de7a27349c4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1800.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T15:49:55.393040Z",
     "start_time": "2025-03-29T15:49:55.366361Z"
    }
   },
   "cell_type": "code",
   "source": [
    "models_nicknames = ['AffinityPropagation',\n",
    " 'AverageAgglomerativeClustering',\n",
    " 'Clique',\n",
    " 'CompleteAgglomerativeClustering',\n",
    " 'DBSCAN',\n",
    " 'HDBSCAN',\n",
    " 'IRFLLRR',\n",
    " 'KMeansProj',\n",
    " 'MeanShift',\n",
    " 'OPTICS',\n",
    " 'Proclus',\n",
    " 'SingleAgglomerativeClustering',\n",
    " 'SpectralClustering',\n",
    " 'SpectralSubspaceRandomization',\n",
    " 'WardAgglomerativeClustering']\n",
    "datasets_ids = \"554,40996\"\n",
    "direction = 'maximize'\n",
    "hpo_metric = 'adjusted_rand'\n",
    "n_trials = '30'\n",
    "pruner = 'none'\n",
    "timeout_hpo = 3600*5\n",
    "timeout_trial = 1800\n",
    "suffix = f'{hpo_metric}'\n",
    "# calculate job arrays\n",
    "combinations = product(datasets_ids.split(','))\n",
    "n_combinations = len(list(combinations))\n",
    "array_start = 0\n",
    "array_end = n_combinations - 1\n",
    "array_step = 1\n",
    "array_simultaneous = min(10, n_combinations)\n",
    "array_batch = array_simultaneous\n",
    "array_fist_start = 0\n",
    "array_fist_end = array_simultaneous - 1\n",
    "for model in models_nicknames:\n",
    "    sampler = models_samplers[model]\n",
    "    if model.find('RecursiveClustering')!=-1:\n",
    "        # we will use the threads of the main process to accelerate\n",
    "        n_jobs = 1\n",
    "    else:\n",
    "        n_jobs = 2\n",
    "    file = script_dir / f'openml_{model}_{suffix}.sbatch'\n",
    "    file_content=f\"\"\"#!/bin/bash\n",
    "#SBATCH -A gen15860@rome\n",
    "#SBATCH -p rome\n",
    "#SBATCH --licenses=fs_unshare,fs_work,fs_scratch\n",
    "#SBATCH --job-name={file.name[:-len('.sbatch')]}\n",
    "#SBATCH --qos=normal\n",
    "#SBATCH -c 60\n",
    "#SBATCH --output=/ccc/cont003/home/dauphine/beluccib/gen15860_beluccib_scratch/recursive_clustering/results/openml/sbatch_output/%x-%A_%a.out\n",
    "#SBATCH --error=/ccc/cont003/home/dauphine/beluccib/gen15860_beluccib_scratch/recursive_clustering/results/openml/sbatch_error/%x-%A_%a.err\n",
    "#SBATCH --time=1-00:00:00\n",
    "#SBATCH --array={array_fist_start}-{array_fist_end}:{array_step}%{array_simultaneous}\n",
    "\n",
    "export OPENML_SKIP_PARQUET=\"true\"\n",
    "export OMP_NUM_THREADS=4\n",
    "export MKL_NUM_THREADS=4\n",
    "export OPENBLAS_NUM_THREADS=4\n",
    "\n",
    "# we are limited at 300 submitted jobs, so we will submit small array jobs and the last array will resubmit the next array job\n",
    "array_start={array_start}\n",
    "array_end={array_end}\n",
    "array_step={array_step}\n",
    "array_simultaneous={array_simultaneous}\n",
    "array_batch={array_batch}\n",
    "script_path=\"/ccc/cont003/home/dauphine/beluccib/gen15860_beluccib_work/recursive_clustering/run_scripts/openml/${{SLURM_JOB_NAME}}.sbatch\"\n",
    "\n",
    "\n",
    "# 1 model, 64 datasets, 10 fold -> 640 models, but slurm is configured to accept at maximum 1001 (array from 0-1000)\n",
    "# then copy another template or modify this one accordingly\n",
    "\n",
    "experiment_root_dir=\"/ccc/cont003/home/dauphine/beluccib/gen15860_beluccib_scratch/recursive_clustering/results/openml\"\n",
    "experiment_work_root_dir=\"/ccc/cont003/home/dauphine/beluccib/gen15860_beluccib_scratch/recursive_clustering/results/openml/${{SLURM_JOB_NAME}}\"\n",
    "experiment_save_root_dir=\"/ccc/cont003/home/dauphine/beluccib/gen15860_beluccib_work/recursive_clustering/results/openml/${{SLURM_JOB_NAME}}\"\n",
    "\n",
    "# For job step/array\n",
    "srun_output=\"${{experiment_root_dir}}/sbatch_output/srun-%x-%A_%a.out\"\n",
    "srun_error=\"${{experiment_root_dir}}/sbatch_error/srun-%x-%A_%a.err\"\n",
    "dbs_dir=\"${{experiment_work_root_dir}}/dbs\"\n",
    "mkdir -p $dbs_dir\n",
    "\n",
    "# then copy another template or modify this one accordingly\n",
    "environment_name=\"cohirf\"\n",
    "experiment_python_location=\"/ccc/cont003/home/dauphine/beluccib/gen15860_beluccib_work/recursive_clustering/recursive_clustering/experiment/hpo_open_ml_clustering_experiment.py\"\n",
    "\n",
    "# Create a dictionary with argument names and values\n",
    "declare -A args_dict=(\n",
    "# base\n",
    "[\"models_nickname\"]=\"{model}\"\n",
    "[\"seeds_models\"]=\"\"\n",
    "[\"experiment_name\"]=\"${{SLURM_JOB_NAME}}\"\n",
    "[\"n_jobs\"]=\"{n_jobs}\"\n",
    "[\"models_params\"]=''\n",
    "[\"fits_params\"]=\"\"\n",
    "[\"error_score\"]=\"\"\n",
    "[\"timeout_fit\"]=\"\"\n",
    "[\"timeout_combination\"]=\"\"\n",
    "[\"log_dir\"]=\"${{experiment_work_root_dir}}/logs\"\n",
    "#[\"log_file_name\"]=\"\"\n",
    "[\"work_root_dir\"]=\"${{experiment_work_root_dir}}/work\"\n",
    "[\"save_root_dir\"]=\"\"\n",
    "#[\"mlflow_tracking_uri\"]=\"\"\n",
    "[\"dask_cluster_type\"]=\"\"\n",
    "[\"n_workers\"]=\"\"\n",
    "[\"n_cores_per_worker\"]=\"\"\n",
    "[\"n_processes_per_worker\"]=\"\"\n",
    "[\"n_threads_per_worker\"]=\"\"\n",
    "[\"n_cores_per_task\"]=\"\"\n",
    "[\"n_processes_per_task\"]=\"\"\n",
    "[\"n_threads_per_task\"]=\"\"\n",
    "[\"dask_memory\"]=\"\"\n",
    "[\"dask_job_extra_directives\"]=\"\"\n",
    "[\"dask_address\"]=\"\"\n",
    "[\"n_gpus_per_worker\"]=\"\"\n",
    "[\"n_gpus_per_task\"]=\"\"\n",
    "# hpo\n",
    "[\"hpo_framework\"]=\"\"\n",
    "[\"n_trials\"]=\"{n_trials}\"\n",
    "[\"timeout_hpo\"]=\"{timeout_hpo}\"\n",
    "[\"timeout_trial\"]=\"{timeout_trial}\"\n",
    "[\"max_concurrent_trials\"]=\"\"\n",
    "[\"sampler\"]=\"{sampler}\"\n",
    "[\"pruner\"]=\"{pruner}\"\n",
    "[\"direction\"]=\"{direction}\"\n",
    "[\"hpo_metric\"]=\"{hpo_metric}\"\n",
    "# openml\n",
    ")\n",
    "\n",
    "declare -A bool_args_dict=(\n",
    "# base\n",
    "[\"create_validation_set\"]=0\n",
    "[\"do_not_clean_work_dir\"]=0\n",
    "[\"do_not_log_to_mlflow\"]=0\n",
    "[\"do_not_check_if_exists\"]=0\n",
    "[\"do_not_retry_on_oom\"]=0\n",
    "[\"raise_on_fit_error\"]=0\n",
    "[\"standardize\"]=1\n",
    ")\n",
    "\n",
    "declare -A array_args_dict=(\n",
    "# Note that bash does not allow arrays inside dictionaries, so we will use strings with ',' as separators and '-' when\n",
    "# we want multiple values for the same argument (basically they will be replaced by ' ')\n",
    "# base\n",
    "[\"datasets_ids\"]=\"{datasets_ids}\"\n",
    "# in this example we are expecting the combinations Model1+0-1, Model1+2-3, Model1+4, Model2+0-1, Model2+2-3, Model2+4\n",
    ")\n",
    "\n",
    "# bash does not necessarily keep the order of the keys in the dictionary, so we will specify the order here\n",
    "declare -a array_args_dict_order=(\"datasets_ids\")\n",
    "\n",
    "# Construct the argument string\n",
    "args_str=\"\"\n",
    "for key in \"${{!args_dict[@]}}\"; do\n",
    "  if [ -n \"${{args_dict[$key]}}\" ]; then\n",
    "    args_str=\"$args_str --$key ${{args_dict[$key]}}\"\n",
    "  fi\n",
    "done\n",
    "\n",
    "# Add arguments strings that are boolean\n",
    "for key in \"${{!bool_args_dict[@]}}\"; do\n",
    "  if [ \"${{bool_args_dict[$key]}}\" -eq 1 ]; then\n",
    "    args_str=\"${{args_str}} --${{key}}\"\n",
    "  fi\n",
    "done\n",
    "\n",
    "# Construct the cartesian product of the arrays\n",
    "# the idea is to create a string like {{Model1,Model2}}+{{0-1,2-3,4}} and then evaluate it to get the cartesian product\n",
    "# using bash's brace expansion\n",
    "string_for_cartesian_product=\"\"\n",
    "for key in \"${{array_args_dict_order[@]}}\"; do\n",
    "  str_array=${{array_args_dict[$key]}}\n",
    "  n_elements=$(echo $str_array | tr ',' ' ' | wc -w)\n",
    "  if [ ${{n_elements}} -eq 0 ]; then\n",
    "    continue\n",
    "  elif [ ${{n_elements}} -eq 1 ]; then\n",
    "    string_for_cartesian_product=\"${{string_for_cartesian_product}}+${{str_array}}\"\n",
    "  else\n",
    "    string_for_cartesian_product=\"${{string_for_cartesian_product}}+{{${{str_array}}}}\"\n",
    "  fi\n",
    "done\n",
    "\n",
    "# Remove the first '+' character\n",
    "string_for_cartesian_product=${{string_for_cartesian_product:1}}\n",
    "\n",
    "# Evaluate the string to get the cartesian product\n",
    "cartesian_product=$(eval echo $string_for_cartesian_product)\n",
    "\n",
    "# Split the string into an array (1 combination per element)\n",
    "IFS=' ' read -r -a cartesian_product <<< \"${{cartesian_product}}\"\n",
    "# cartesian_product is now an array like [\"Model1+0-1\", \"Model1+2-3\", \"Model1+4\", \"Model2+0-1\", \"Model2+2-3\", \"Model2+4\"]\n",
    "\n",
    "# Activate the conda environment\n",
    "eval \"$(conda shell.bash hook)\"\n",
    "conda activate $environment_name\n",
    "\n",
    "# alternatively we could also run an array job and execute the combination given by the SLURM_ARRAY_TASK_ID\n",
    "i_combination=$SLURM_ARRAY_TASK_ID\n",
    "if [ \"${{i_combination}}\" -ge \"${{#cartesian_product[@]}}\" ]; then\n",
    "  echo \"SLURM_ARRAY_TASK_ID is greater than the number of combinations\"\n",
    "  exit 1\n",
    "fi\n",
    "string_combination=\"\"\n",
    "IFS='+' read -r -a combination <<< \"${{cartesian_product[$i_combination]}}\"\n",
    "i_arg_name=0\n",
    "for key in \"${{array_args_dict_order[@]}}\"; do\n",
    "  value=${{combination[$i_arg_name]//[-]/ }}  # replace '-' by ' '\n",
    "  string_combination=\"${{string_combination}} --${{key}} ${{value}}\"\n",
    "  i_arg_name=$((i_arg_name+1))\n",
    "done\n",
    "mlflow_tracking_uri=\"sqlite:////${{dbs_dir}}/${{cartesian_product[$i_combination]}}.db\"\n",
    "log_file_name=\"${{SLURM_JOB_NAME}}-${{cartesian_product[$i_combination]}}\"\n",
    "echo \"Running: srun --exclusive -n 1 -c ${{SLURM_CPUS_PER_TASK}} --output=${{srun_output}} --error=${{srun_error}} python ${{experiment_python_location}} ${{args_str}} ${{string_combination}} --mlflow_tracking_uri ${{mlflow_tracking_uri}} --log_file_name ${{log_file_name}}\"\n",
    "srun --exclusive -n 1 -c ${{SLURM_CPUS_PER_TASK}} --output=${{srun_output}} --error=${{srun_error}} python ${{experiment_python_location}} ${{args_str}} ${{string_combination}} --mlflow_tracking_uri ${{mlflow_tracking_uri}} --log_file_name ${{log_file_name}}\n",
    "wait\n",
    "\n",
    "# If we are at the last array task, submit the next array job\n",
    "if [ \"${{SLURM_ARRAY_TASK_ID}}\" -eq \"${{SLURM_ARRAY_TASK_MAX}}\" ]; then\n",
    "  next_array_start=$(($SLURM_ARRAY_TASK_MIN+array_batch))\n",
    "  # next_array_end will be the minimum between array_end and next_array_start+array_batch\n",
    "  next_array_end=$(( array_end < next_array_start+array_batch ? array_end : next_array_start+array_batch ))\n",
    "  # if next_array_start <= array_end, then we submit job, otherwise we are done\n",
    "  if [ \"${{next_array_start}}\" -le \"${{next_array_end}}\" ]; then\n",
    "    sbatch --array=${{next_array_start}}-${{next_array_end}}:${{array_step}}%${{array_simultaneous}} ${{script_path}}\n",
    "  fi\n",
    "fi\n",
    "\"\"\"\n",
    "    file.write_text(file_content)\n"
   ],
   "id": "86ce89ff1abd7aef",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c79f9d5ce5ba0f48"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cohirf",
   "language": "python",
   "name": "cohirf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
