{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-28T13:20:55.659738Z",
     "start_time": "2025-03-28T13:20:54.629399Z"
    }
   },
   "source": [
    "from recursive_clustering.experiment.tested_models import models_dict\n",
    "from pathlib import Path\n",
    "from itertools import product"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T13:20:55.672114Z",
     "start_time": "2025-03-28T13:20:55.665483Z"
    }
   },
   "cell_type": "code",
   "source": [
    "models_nicknames = list(models_dict.keys())\n",
    "script_dir = Path() / 'scripts'\n",
    "script_dir.mkdir(parents=True, exist_ok=True)\n",
    "models_nicknames.remove('WBMS')\n",
    "models_nicknames.sort()\n",
    "models_nicknames"
   ],
   "id": "3f98c393bb9116ba",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AffinityPropagation',\n",
       " 'AverageAgglomerativeClustering',\n",
       " 'Clique',\n",
       " 'CompleteAgglomerativeClustering',\n",
       " 'DBSCAN',\n",
       " 'HDBSCAN',\n",
       " 'IRFLLRR',\n",
       " 'KMeans',\n",
       " 'KMeansProj',\n",
       " 'MeanShift',\n",
       " 'OPTICS',\n",
       " 'Proclus',\n",
       " 'RecursiveClustering',\n",
       " 'RecursiveClustering_full',\n",
       " 'SingleAgglomerativeClustering',\n",
       " 'SpectralClustering',\n",
       " 'SpectralSubspaceRandomization',\n",
       " 'WardAgglomerativeClustering']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T13:20:55.784474Z",
     "start_time": "2025-03-28T13:20:55.781639Z"
    }
   },
   "cell_type": "code",
   "source": "len(models_nicknames)",
   "id": "5168dc77fd183100",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T13:20:55.829102Z",
     "start_time": "2025-03-28T13:20:55.825477Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# sensible parameters for n_classes=5\n",
    "models_params = {\n",
    "    'AffinityPropagation': {\"damping\": 0.9},\n",
    "    'AverageAgglomerativeClustering': {'n_clusters': 5},\n",
    "    'Clique': {},\n",
    "    'CompleteAgglomerativeClustering': {'n_clusters': 5},\n",
    "    'DBSCAN': {},\n",
    "    'HDBSCAN': {},\n",
    "    'IRFLLRR': {'sc_n_clusters': 5},\n",
    "    'KMeans': {'n_clusters': 5},\n",
    "    'KMeansProj': {},\n",
    "    'MeanShift': {},\n",
    "    'OPTICS': {},\n",
    "    'Proclus': {'n_clusters': 5},\n",
    "    'RecursiveClustering': {},\n",
    "    'RecursiveClustering_full': {},\n",
    "    'SingleAgglomerativeClustering': {'n_clusters': 5},\n",
    "    'SpectralClustering': {'n_clusters': 5},\n",
    "    'SpectralSubspaceRandomization': {'sc_n_clusters': 5},\n",
    "    'WardAgglomerativeClustering': {'n_clusters': 5},\n",
    "}"
   ],
   "id": "292e1134d4d9243e",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T13:24:44.683909Z",
     "start_time": "2025-03-28T13:24:44.673629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# models_nicknames = ['AffinityPropagation', 'AverageAgglomerativeClustering', 'Clique', 'CompleteAgglomerativeClustering', 'DBSCAN', 'HDBSCAN','KMeans', 'KMeansProj', 'MeanShift', 'OPTICS', 'Proclus', 'RecursiveClustering', 'SingleAgglomerativeClustering']\n",
    "# ok = ['Clique', 'DBSCAN', 'KMeans','RecursiveClustering',]\n",
    "models_nicknames = ['IRFLLRR']\n",
    "n_samples = \"14427\"\n",
    "n_features = \"50000\"\n",
    "# n_samples = \"100,347,1202,4163,14427,50000\"\n",
    "# n_features = \"14427\"\n",
    "# n_features = \"100,347,1202,4163,14427,50000\"\n",
    "# n_samples = \"14427\"\n",
    "# seeds_unified = \"3,4\"\n",
    "seeds_unified = \"1\"\n",
    "suffix = 'features'\n",
    "# suffix = 'samples'\n",
    "# calculate job arrays\n",
    "combinations = product(n_samples.split(','), n_features.split(','), seeds_unified.split(','))\n",
    "n_combinations = len(list(combinations))\n",
    "array_start = 0\n",
    "array_end = n_combinations - 1\n",
    "array_step = 1\n",
    "array_simultaneous = n_combinations\n",
    "array_batch = array_simultaneous\n",
    "array_fist_start = 0\n",
    "array_fist_end = array_simultaneous - 1\n",
    "for model in models_nicknames:\n",
    "    if model == 'RecursiveClustering':\n",
    "        # we will use the threads of the main process to accelerate\n",
    "        n_jobs = 1\n",
    "    else:\n",
    "        n_jobs = 2\n",
    "    file = script_dir / f'time_hc_{model}_{suffix}.sbatch'\n",
    "    file_content=f\"\"\"#!/bin/bash\n",
    "#SBATCH -A gen15860@rome\n",
    "#SBATCH -p rome\n",
    "#SBATCH --licenses=fs_unshare,fs_work,fs_scratch\n",
    "#SBATCH --job-name=time_hc_{model}_{suffix}\n",
    "#SBATCH --qos=normal\n",
    "#SBATCH -c 128\n",
    "#SBATCH --output=/ccc/cont003/home/dauphine/beluccib/gen15860_beluccib_scratch/recursive_clustering/results/classif_time/sbatch_output/%x-%A_%a.out\n",
    "#SBATCH --error=/ccc/cont003/home/dauphine/beluccib/gen15860_beluccib_scratch/recursive_clustering/results/classif_time/sbatch_error/%x-%A_%a.err\n",
    "#SBATCH --time=1-00:00:00\n",
    "#SBATCH --array={array_fist_start}-{array_fist_end}:{array_step}%{array_simultaneous}\n",
    "\n",
    "export OMP_NUM_THREADS=4\n",
    "export MKL_NUM_THREADS=4\n",
    "export OPENBLAS_NUM_THREADS=4\n",
    "\n",
    "# we are limited at 300 submitted jobs, so we will submit small array jobs and the last array will resubmit the next array job\n",
    "array_start={array_start}\n",
    "array_end={array_end}\n",
    "array_step={array_step}\n",
    "array_simultaneous={array_simultaneous}\n",
    "array_batch={array_batch}\n",
    "script_path=\"/ccc/cont003/home/dauphine/beluccib/gen15860_beluccib_work/recursive_clustering/run_scripts/classif_time/${{SLURM_JOB_NAME}}.sbatch\"\n",
    "\n",
    "\n",
    "# 1 model, 64 datasets, 10 fold -> 640 models, but slurm is configured to accept at maximum 1001 (array from 0-1000)\n",
    "# then copy another template or modify this one accordingly\n",
    "\n",
    "experiment_root_dir=\"/ccc/cont003/home/dauphine/beluccib/gen15860_beluccib_scratch/recursive_clustering/results/classif_time\"\n",
    "experiment_work_root_dir=\"/ccc/cont003/home/dauphine/beluccib/gen15860_beluccib_scratch/recursive_clustering/results/classif_time/${{SLURM_JOB_NAME}}\"\n",
    "experiment_save_root_dir=\"/ccc/cont003/home/dauphine/beluccib/gen15860_beluccib_work/recursive_clustering/results/classif_time/${{SLURM_JOB_NAME}}\"\n",
    "\n",
    "# For job step/array\n",
    "srun_output=\"${{experiment_root_dir}}/sbatch_output/srun-%x-%A_%a.out\"\n",
    "srun_error=\"${{experiment_root_dir}}/sbatch_error/srun-%x-%A_%a.err\"\n",
    "dbs_dir=\"${{experiment_work_root_dir}}/dbs\"\n",
    "mkdir -p $dbs_dir\n",
    "\n",
    "# then copy another template or modify this one accordingly\n",
    "environment_name=\"cohirf\"\n",
    "experiment_python_location=\"/ccc/cont003/home/dauphine/beluccib/gen15860_beluccib_work/recursive_clustering/recursive_clustering/experiment/classification_clustering_experiment.py\"\n",
    "\n",
    "# Create a dictionary with argument names and values\n",
    "declare -A args_dict=(\n",
    "# base\n",
    "[\"models_nickname\"]=\"{model}\"\n",
    "[\"seeds_models\"]=\"\"\n",
    "[\"experiment_name\"]=\"${{SLURM_JOB_NAME}}\"\n",
    "[\"n_jobs\"]=\"{n_jobs}\"\n",
    "[\"models_params\"]='{str(models_params[model]).replace(' ', '').replace(\"'\", '\"')}'\n",
    "[\"fits_params\"]=\"\"\n",
    "[\"error_score\"]=\"\"\n",
    "[\"timeout_fit\"]=\"36000\"\n",
    "[\"timeout_combination\"]=\"\"\n",
    "[\"log_dir\"]=\"${{experiment_work_root_dir}}/logs\"\n",
    "#[\"log_file_name\"]=\"\"\n",
    "[\"work_root_dir\"]=\"${{experiment_work_root_dir}}/work\"\n",
    "[\"save_root_dir\"]=\"\"\n",
    "#[\"mlflow_tracking_uri\"]=\"\"\n",
    "[\"dask_cluster_type\"]=\"\"\n",
    "[\"n_workers\"]=\"\"\n",
    "[\"n_cores_per_worker\"]=\"\"\n",
    "[\"n_processes_per_worker\"]=\"\"\n",
    "[\"n_threads_per_worker\"]=\"\"\n",
    "[\"n_cores_per_task\"]=\"\"\n",
    "[\"n_processes_per_task\"]=\"\"\n",
    "[\"n_threads_per_task\"]=\"\"\n",
    "[\"dask_memory\"]=\"\"\n",
    "[\"dask_job_extra_directives\"]=\"\"\n",
    "[\"dask_address\"]=\"\"\n",
    "[\"n_gpus_per_worker\"]=\"\"\n",
    "[\"n_gpus_per_task\"]=\"\"\n",
    "# hpo\n",
    "[\"hpo_framework\"]=\"\"\n",
    "[\"n_trials\"]=\"\"\n",
    "[\"timeout_hpo\"]=\"\"\n",
    "[\"timeout_trial\"]=\"\"\n",
    "[\"max_concurrent_trials\"]=\"\"\n",
    "[\"sampler\"]=\"\"\n",
    "[\"pruner\"]=\"\"\n",
    "[\"direction\"]=\"\"\n",
    "[\"hpo_metric\"]=\"\"\n",
    "# classification clustering\n",
    "[\"n_random\"]=\"\"\n",
    "[\"n_informative\"]=\"\"\n",
    "[\"n_redundant\"]=\"\"\n",
    "[\"n_repeated\"]=\"\"\n",
    "[\"n_classes\"]=\"\"\n",
    "[\"n_clusters_per_class\"]=\"\"\n",
    "[\"weights\"]=\"\"\n",
    "[\"flip_y\"]=\"\"\n",
    "[\"class_sep\"]=\"100\"\n",
    "[\"hypercube\"]=\"\"\n",
    "[\"shift\"]=\"\"\n",
    "[\"scale\"]=\"\"\n",
    "[\"shuffle\"]=\"\"\n",
    "[\"seeds_dataset\"]=\"\"\n",
    "[\"pct_random\"]=\"0.0\"\n",
    "[\"add_outlier\"]=\"\"\n",
    ")\n",
    "\n",
    "declare -A bool_args_dict=(\n",
    "# base\n",
    "[\"create_validation_set\"]=0\n",
    "[\"do_not_clean_work_dir\"]=0\n",
    "[\"do_not_log_to_mlflow\"]=0\n",
    "[\"do_not_check_if_exists\"]=0\n",
    "[\"do_not_retry_on_oom\"]=0\n",
    "[\"raise_on_fit_error\"]=0\n",
    ")\n",
    "\n",
    "declare -A array_args_dict=(\n",
    "# Note that bash does not allow arrays inside dictionaries, so we will use strings with ',' as separators and '-' when\n",
    "# we want multiple values for the same argument (basically they will be replaced by ' ')\n",
    "# base\n",
    "[\"n_samples\"]=\"{n_samples}\"\n",
    "[\"n_features\"]=\"{n_features}\"\n",
    "[\"seeds_unified\"]=\"{seeds_unified}\"\n",
    "# in this example we are expecting the combinations Model1+0-1, Model1+2-3, Model1+4, Model2+0-1, Model2+2-3, Model2+4\n",
    ")\n",
    "\n",
    "# bash does not necessarily keep the order of the keys in the dictionary, so we will specify the order here\n",
    "declare -a array_args_dict_order=(\"n_samples\" \"n_features\" \"seeds_unified\")\n",
    "\n",
    "# Construct the argument string\n",
    "args_str=\"\"\n",
    "for key in \"${{!args_dict[@]}}\"; do\n",
    "  if [ -n \"${{args_dict[$key]}}\" ]; then\n",
    "    args_str=\"$args_str --$key ${{args_dict[$key]}}\"\n",
    "  fi\n",
    "done\n",
    "\n",
    "# Add arguments strings that are boolean\n",
    "for key in \"${{!bool_args_dict[@]}}\"; do\n",
    "  if [ \"${{bool_args_dict[$key]}}\" -eq 1 ]; then\n",
    "    args_str=\"${{args_str}} --${{key}}\"\n",
    "  fi\n",
    "done\n",
    "\n",
    "# Construct the cartesian product of the arrays\n",
    "# the idea is to create a string like {{Model1,Model2}}+{{0-1,2-3,4}} and then evaluate it to get the cartesian product\n",
    "# using bash's brace expansion\n",
    "string_for_cartesian_product=\"\"\n",
    "for key in \"${{array_args_dict_order[@]}}\"; do\n",
    "  str_array=${{array_args_dict[$key]}}\n",
    "  n_elements=$(echo $str_array | tr ',' ' ' | wc -w)\n",
    "  if [ ${{n_elements}} -eq 0 ]; then\n",
    "    continue\n",
    "  elif [ ${{n_elements}} -eq 1 ]; then\n",
    "    string_for_cartesian_product=\"${{string_for_cartesian_product}}+${{str_array}}\"\n",
    "  else\n",
    "    string_for_cartesian_product=\"${{string_for_cartesian_product}}+{{${{str_array}}}}\"\n",
    "  fi\n",
    "done\n",
    "\n",
    "# Remove the first '+' character\n",
    "string_for_cartesian_product=${{string_for_cartesian_product:1}}\n",
    "\n",
    "# Evaluate the string to get the cartesian product\n",
    "cartesian_product=$(eval echo $string_for_cartesian_product)\n",
    "\n",
    "# Split the string into an array (1 combination per element)\n",
    "IFS=' ' read -r -a cartesian_product <<< \"${{cartesian_product}}\"\n",
    "# cartesian_product is now an array like [\"Model1+0-1\", \"Model1+2-3\", \"Model1+4\", \"Model2+0-1\", \"Model2+2-3\", \"Model2+4\"]\n",
    "\n",
    "# Activate the conda environment\n",
    "eval \"$(conda shell.bash hook)\"\n",
    "conda activate $environment_name\n",
    "\n",
    "# alternatively we could also run an array job and execute the combination given by the SLURM_ARRAY_TASK_ID\n",
    "i_combination=$SLURM_ARRAY_TASK_ID\n",
    "if [ \"${{i_combination}}\" -ge \"${{#cartesian_product[@]}}\" ]; then\n",
    "  echo \"SLURM_ARRAY_TASK_ID is greater than the number of combinations\"\n",
    "  exit 1\n",
    "fi\n",
    "string_combination=\"\"\n",
    "IFS='+' read -r -a combination <<< \"${{cartesian_product[$i_combination]}}\"\n",
    "i_arg_name=0\n",
    "for key in \"${{array_args_dict_order[@]}}\"; do\n",
    "  value=${{combination[$i_arg_name]//[-]/ }}  # replace '-' by ' '\n",
    "  string_combination=\"${{string_combination}} --${{key}} ${{value}}\"\n",
    "  i_arg_name=$((i_arg_name+1))\n",
    "done\n",
    "mlflow_tracking_uri=\"sqlite:////${{dbs_dir}}/${{cartesian_product[$i_combination]}}.db\"\n",
    "log_file_name=\"${{SLURM_JOB_NAME}}-${{cartesian_product[$i_combination]}}\"\n",
    "echo \"Running: srun --exclusive -n 1 -c ${{SLURM_CPUS_PER_TASK}} --output=${{srun_output}} --error=${{srun_error}} python ${{experiment_python_location}} ${{args_str}} ${{string_combination}} --mlflow_tracking_uri ${{mlflow_tracking_uri}} --log_file_name ${{log_file_name}}\"\n",
    "srun --exclusive -n 1 -c ${{SLURM_CPUS_PER_TASK}} --output=${{srun_output}} --error=${{srun_error}} python ${{experiment_python_location}} ${{args_str}} ${{string_combination}} --mlflow_tracking_uri ${{mlflow_tracking_uri}} --log_file_name ${{log_file_name}}\n",
    "wait\n",
    "\n",
    "# If we are at the last array task, submit the next array job\n",
    "if [ \"${{SLURM_ARRAY_TASK_ID}}\" -eq \"${{SLURM_ARRAY_TASK_MAX}}\" ]; then\n",
    "  next_array_start=$(($SLURM_ARRAY_TASK_MIN+array_batch))\n",
    "  # next_array_end will be the minimum between array_end and next_array_start+array_batch\n",
    "  next_array_end=$(( array_end < next_array_start+array_batch ? array_end : next_array_start+array_batch ))\n",
    "  # if next_array_start <= array_end, then we submit job, otherwise we are done\n",
    "  if [ \"${{next_array_start}}\" -le \"${{next_array_end}}\" ]; then\n",
    "    sbatch --array=${{next_array_start}}-${{next_array_end}}:${{array_step}}%${{array_simultaneous}} ${{script_path}}\n",
    "  fi\n",
    "fi\n",
    "\"\"\"\n",
    "    file.write_text(file_content)\n"
   ],
   "id": "86ce89ff1abd7aef",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "654167c1f9790c64"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cohirf",
   "language": "python",
   "name": "cohirf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
